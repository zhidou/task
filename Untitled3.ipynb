{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import functions as func\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 16\n",
    "# mini_batch = 16\n",
    "# train_iter = batch_size//mini_batch\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "para = [weights['wc1'], weights['wc2'], weights['wd1'], weights['out'], biases['bc1'], biases['bc2'], biases['bd1'], biases['out']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------------------------------define graph------------------------------------\n",
    "# Reshape input picture\n",
    "inputx = tf.reshape(x, shape=[batch_size, 28, 28, 1])\n",
    "yi = y\n",
    "\n",
    "# ------------------------------The tf defined network--------------------------\n",
    "# conv11 = func.conv2d(inputx, weights['wc1'], biases['bc1'])\n",
    "# pool11 = tf.nn.max_pool(conv11, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "# conv22 = func.conv2d(pool11, weights['wc2'], biases['bc2'])\n",
    "# pool22 = tf.nn.max_pool(conv22, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "# fcc = tf.reshape(pool22, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "# fc11 = tf.add(tf.matmul(fcc, weights['wd1']), biases['bd1'])\n",
    "# fc11 = tf.nn.relu(fc11)\n",
    "\n",
    "# pred1 = tf.add(tf.matmul(fc11, weights['out']), biases['out'])\n",
    "\n",
    "# ------------------------------self defined network-----------------------------\n",
    "# Convolution Layer\n",
    "conv1 = func.conv2d(inputx, weights['wc1'], biases['bc1'])\n",
    "# Pooling (down-sampling)\n",
    "p1 = func.extract_patches(conv1, 'SAME', 2, 2)\n",
    "f1 = func.majority_frequency(p1)\n",
    "#     maxpooling\n",
    "pool1, mask1 = func.weight_pool_with_mask(p1, f1, pool_fun=func.majority_pool_with_mask, reduce_fun=tf.reduce_max)\n",
    "\n",
    "# Convolution Layer\n",
    "conv2 = func.conv2d(pool1, weights['wc2'], biases['bc2'])\n",
    "#     Pooling (down-sampling)\n",
    "p2 = func.extract_patches(conv2, 'SAME', 2, 2)\n",
    "f2 = func.majority_frequency(p2)\n",
    "#     maxpooling\n",
    "pool2, mask2 = func.weight_pool_with_mask(p2, f2, pool_fun=func.majority_pool_with_mask, reduce_fun=tf.reduce_max)\n",
    "\n",
    "# Fully connected layer\n",
    "# Reshape conv2 output to fit fully connected layer input\n",
    "fc = tf.reshape(pool2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "fc1 = tf.add(tf.matmul(fc, weights['wd1']), biases['bd1'])\n",
    "fc1 = tf.nn.relu(fc1)\n",
    "# Apply Dropout\n",
    "# fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "# Output, class prediction\n",
    "pred = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "correct_pred = tf.cast(tf.equal(tf.argmax(pred, 1), tf.argmax(yi, 1)), dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------define graph------------------------------------\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "gv = opt.compute_gradients(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)))\n",
    "# ------------------------------define gradient descent-------------------------\n",
    "\n",
    "# the last fc\n",
    "e = tf.nn.softmax(pred) - yi\n",
    "grad_w_out = tf.transpose(fc1) @ e\n",
    "grad_b_out = tf.reduce_sum(e, axis=0)\n",
    "\n",
    "# the second last fc\n",
    "# we use droupout at the last second layer, then we should just update the nodes that are active\n",
    "e = tf.multiply(e @ tf.transpose(weights['out']), tf.cast(tf.greater(fc1, 0), dtype=tf.float32)) #/ dropout\n",
    "grad_w_3 = tf.transpose(fc) @ e\n",
    "grad_b_3 = tf.reduce_sum(e, axis=0)\n",
    "\n",
    "# the last pooling layer\n",
    "e = e @ tf.transpose(weights['wd1'])\n",
    "e = tf.reshape(e, pool2.get_shape().as_list())\n",
    "\n",
    "# the last conv layer\n",
    "# unpooling get error from pooling layer\n",
    "e = func.error_pooling2conv(e, mask2)\n",
    "\n",
    "# multiply with the derivative of the active function on the conv layer\n",
    "#     this one is also important this is a part from the upsampling, but \n",
    "e = tf.multiply(e, tf.cast(tf.greater(conv2, 0), dtype=tf.float32))\n",
    "temp1, temp2 = func.filter_gradient(e, pool1, conv2)\n",
    "grad_k_2 = temp1\n",
    "grad_b_2 = temp2\n",
    "\n",
    "# conv to pool\n",
    "e = func.error_conv2pooling(e, weights['wc2'])\n",
    "\n",
    "# pool to the first conv\n",
    "e = func.error_pooling2conv(e, mask1)\n",
    "e = tf.multiply(e, tf.cast(tf.greater(conv1, 0), dtype=tf.float32))\n",
    "temp1, temp2 = func.filter_gradient(e, inputx, conv1)\n",
    "grad_k_1 = temp1\n",
    "grad_b_1 = temp2\n",
    "    \n",
    "    \n",
    "\n",
    "# gradient\n",
    "gv1 = [(grad_k_1, weights['wc1']), (grad_k_2, weights['wc2']), \n",
    "       (grad_w_3 / batch_size, weights['wd1']), (grad_w_out / batch_size, weights['out']),\n",
    "       (grad_b_1, biases['bc1']), (grad_b_2, biases['bc2']), \n",
    "       (grad_b_3 / batch_size, biases['bd1']), (grad_b_out / batch_size, biases['out'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# f = open('output.txt', 'w')\n",
    "# Launch the graph \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "#     while step * batch_size < training_iters:\n",
    "    while step < 2:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "#         ret1 all from tf.  gv1 all from mime, gv2 half half\n",
    "        ret1, ret2 = sess.run([gv, gv1], feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "#         conv, pool, ee4, ee3 = sess.run([conv2, pool2, e4, e3], feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            acc = sess.run(accuracy,feed_dict={x: batch_x,\n",
    "                                                          y: batch_y,\n",
    "                                                          keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) + \"\\nTraining Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  236.395,    67.118,   245.784,   362.196,   363.377],\n",
       "       [  158.298,   -35.431,   132.62 ,   360.272,   310.257],\n",
       "       [  864.503,   688.634,   865.075,  1098.375,   780.748],\n",
       "       [ 1570.123,  2003.334,  1882.387,  1476.438,  1155.729],\n",
       "       [ 1494.532,  1964.872,  1653.557,  1356.253,   725.913]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret1[0][0][:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  236.395,    67.118,   245.784,   362.196,   363.377],\n",
       "       [  158.298,   -35.431,   132.619,   360.272,   310.257],\n",
       "       [  864.502,   688.634,   865.075,  1098.375,   780.748],\n",
       "       [ 1570.122,  2003.334,  1882.387,  1476.438,  1155.729],\n",
       "       [ 1494.532,  1964.872,  1653.557,  1356.253,   725.913]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret2[0][0][:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference between tf and mine\n",
      "0.060152 0.0\n",
      "0.292672 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0168953 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print('difference between tf and mine')\n",
    "for i, j in zip(ret1, ret2):\n",
    "    print(np.sum(np.abs(i[0] - j[0])), np.sum(np.abs(i[1] - j[1])))\n",
    "# print('difference between tf and half')\n",
    "# for i, k in zip(ret1, ret3):\n",
    "#     print(np.max(i[0] - k[0]), np.sum(np.abs(i[1] - k[1])))\n",
    "# print('difference between mine and half')\n",
    "# for j, k in zip(ret2, ret3):\n",
    "#     print(np.max(j[0] - k[0]), np.sum(np.abs(j[1] - k[1])))\n",
    "# print('difference between tf and tf')\n",
    "# for i, l in zip(ret1, ret4):\n",
    "#     print(np.max(i[0] - l[0]), np.sum(np.abs(i[1] - l[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as f:\n",
    "    f.write('conv2 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, conv[0,:,:,0], delimiter=', ',fmt=\"%.2f\")\n",
    "    \n",
    "with open('output.txt', 'a') as f:\n",
    "    f.write('\\npool2 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, pool[0,:,:,0], delimiter=', ',fmt=\"%.2f\")\n",
    "\n",
    "with open('output.txt', 'a') as f:\n",
    "    f.write('\\nerror4 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, ee4[0,:,:,0], delimiter=', ',fmt=\"%.2f\")\n",
    "with open('output.txt', 'a') as f:\n",
    "    f.write('\\nerror3 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, ee3[0,:,:,0], delimiter=', ',fmt=\"%.2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = np.array([range(1, 65)])\n",
    "tt = np.reshape(tt, [2, 2, 4,4], order='C')\n",
    "tt = np.transpose(tt, [0, 2,3,1])\n",
    "\n",
    "tt[0,0,3,0] = 3\n",
    "tt[0,2,0,0] = 13\n",
    "tt[0,2,1,0] = 14\n",
    "tt[0,2,3,0] = 11\n",
    "tt[0,3,2,0] = 11\n",
    "tt[0,3,3,0] = 11\n",
    "tt[0,0,0,1] = 18\n",
    "tt[0,1,0,1] = 18\n",
    "tt[0,1,1,1] = 18\n",
    "tt[0,0,2,1] = 23\n",
    "tt[0,0,3,1] = 23\n",
    "tt[0,2,0,1] = 30\n",
    "\n",
    "x = tf.constant(tt, dtype=tf.float32)\n",
    "p = func.extract_patches(x, \"VALID\", 2, 2)\n",
    "pool1, mask = func.max_pool_with_mask(p=p)\n",
    "pool2 = tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1],padding='VALID')\n",
    "\n",
    "# x = tf.reshape(x, [4,4])\n",
    "with tf.Session() as sess:\n",
    "    retx, retp, retm = sess.run([x, pool1, mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.,   8.],\n",
       "       [ 14.,  11.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retp[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0. ,  0.5,  0. ,  0.5], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retm[0,1,0,:,0]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs505]",
   "language": "python",
   "name": "conda-env-cs505-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
