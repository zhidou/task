{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import functions as func\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data_x = np.load('../cifar_x.npy')\n",
    "data_y = np.load('../cifar_y.npy')\n",
    "test_x = np.load('../cifar_test_x.npy')\n",
    "test_y = np.load('../cifar_test_y.npy')\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size, data_x = data_x, data_y = data_y):\n",
    "    global index\n",
    "    i = 0\n",
    "    x = []\n",
    "    y = []\n",
    "    while i < batch_size:\n",
    "        if index == len(data_x): index = 0\n",
    "        x.append(data_x[index].tolist())\n",
    "        temp = [0] * 10\n",
    "        temp[data_y[index]] = 1\n",
    "        y.append(temp)\n",
    "        index += 1\n",
    "        i += 1\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "mini_batch = 8\n",
    "train_iter = batch_size//mini_batch\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # Cifar 10 data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 3, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([8*8*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "para = [weights['wc1'], weights['wc2'], weights['wd1'], weights['out'],\n",
    "        biases['bc1'], biases['bc2'], biases['bd1'], biases['out']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# body of network\n",
    "def body2(i, x, out):\n",
    "    # Convolution Layer\n",
    "    inputx = x.read(index=i)\n",
    "    conv1 = func.conv2d(inputx, weights['wc1'], biases['bc1'])\n",
    "    # Pooling (down-sampling)\n",
    "    p1 = func.extract_patches(conv1, 'SAME', 2, 2)\n",
    "    f1 = func.majority_frequency(p1)\n",
    "    #     maxpooling\n",
    "    pool1 = func.max_pool(p=p1)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = func.conv2d(pool1, weights['wc2'], biases['bc2'])\n",
    "    #     Pooling (down-sampling)\n",
    "    p2 = func.extract_patches(conv2, 'SAME', 2, 2)\n",
    "    f2 = func.majority_frequency(p2)\n",
    "    #     maxpooling\n",
    "    pool2 = func.max_pool(p=p2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc = tf.reshape(pool2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    # Output, class prediction\n",
    "    out = out.write(index=i, value=tf.add(tf.matmul(fc1, weights['out']), biases['out']))\n",
    "    i += 1\n",
    "    return i, x, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputxs = tf.TensorArray(dtype=tf.float32, size=train_iter, clear_after_read=True).split(x, np.array([mini_batch] * train_iter))\n",
    "out = tf.TensorArray(dtype=tf.float32, size=train_iter)\n",
    "\n",
    "\n",
    "# compute gradient and update the weights\n",
    "i0 = tf.constant(0)\n",
    "con = lambda i, x, g: i < train_iter\n",
    "\n",
    "i0, inputx, out = tf.while_loop(cond=con, body=body2, loop_vars=[i0, inputxs, out], parallel_iterations=1)\n",
    "\n",
    "pred = tf.reshape(out.stack(), [-1, 10])\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,1), tf.argmax(y,1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "Testing Accuracy: 0.125\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "#     while step * batch_size < training_iters:\n",
    "    while step < 2:\n",
    "        batch_x, batch_y = get_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(opt, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            cos, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                y: batch_y, keep_prob: 1.})\n",
    "            cost_save.append(cos)\n",
    "            accuracy_save.append(acc)\n",
    "\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(cos) +\n",
    "                  \"\\nTraining Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "#     Calculate accuracy for test images\n",
    "    global index\n",
    "    index = 0\n",
    "    batch_x, batch_y = get_batch(batch_size, test_x, test_y)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------define graph------------------------------------\n",
    "# Reshape input picture\n",
    "inputx = tf.reshape(x, shape=[batch_size, 28, 28, 1])\n",
    "yi = y\n",
    "\n",
    "# ------------------------------The tf defined network--------------------------\n",
    "conv11 = func.conv2d(inputx, weights['wc1'], biases['bc1'])\n",
    "pool11 = tf.nn.max_pool(conv11, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "conv22 = func.conv2d(pool11, weights['wc2'], biases['bc2'])\n",
    "pool22 = tf.nn.max_pool(conv22, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "fcc = tf.reshape(pool22, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "fc11 = tf.add(tf.matmul(fcc, weights['wd1']), biases['bd1'])\n",
    "fc11 = tf.nn.relu(fc11)\n",
    "\n",
    "pred1 = tf.add(tf.matmul(fc11, weights['out']), biases['out'])\n",
    "#------------------------------------------------------------------------------------------\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "gv = opt.compute_gradients(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred1, labels=y)))\n",
    "\n",
    "# ------------------------------self defined network-----------------------------\n",
    "# Convolution Layer\n",
    "conv1 = func.conv2d(inputx, weights['wc1'], biases['bc1'])\n",
    "# Pooling (down-sampling)\n",
    "p1 = func.extract_patches(conv1, 'SAME', 2, 2)\n",
    "f1 = func.majority_frequency(p1)\n",
    "#     maxpooling\n",
    "# pool1, mask1 = func.weight_pool_with_mask(p1, f1, pool_fun=func.majority_pool_with_mask, reduce_fun=tf.reduce_max)\n",
    "pool1, mask1 = func.max_pool_with_mask(p1)\n",
    "\n",
    "# Convolution Layer\n",
    "conv2 = func.conv2d(pool1, weights['wc2'], biases['bc2'])\n",
    "#     Pooling (down-sampling)\n",
    "p2 = func.extract_patches(conv2, 'SAME', 2, 2)\n",
    "f2 = func.majority_frequency(p2)\n",
    "#     maxpooling\n",
    "# pool2, mask2 = func.weight_pool_with_mask(p2, f2, pool_fun=func.majority_pool_with_mask, reduce_fun=tf.reduce_max)\n",
    "pool2, mask2 = func.max_pool_with_mask(p2)\n",
    "\n",
    "# Fully connected layer\n",
    "# Reshape conv2 output to fit fully connected layer input\n",
    "fc = tf.reshape(pool2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "fc1 = tf.add(tf.matmul(fc, weights['wd1']), biases['bd1'])\n",
    "fc1 = tf.nn.relu(fc1)\n",
    "# Apply Dropout\n",
    "# fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "# Output, class prediction\n",
    "pred = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "correct_pred = tf.cast(tf.equal(tf.argmax(pred, 1), tf.argmax(yi, 1)), dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------define graph------------------------------------\n",
    "opt1 = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "gv1 = opt1.compute_gradients(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)))\n",
    "# ------------------------------define gradient descent-------------------------\n",
    "\n",
    "# the last fc\n",
    "e = tf.nn.softmax(pred) - yi\n",
    "grad_w_out = tf.transpose(fc1) @ e\n",
    "grad_b_out = tf.reduce_sum(e, axis=0)\n",
    "\n",
    "# the second last fc\n",
    "# we use droupout at the last second layer, then we should just update the nodes that are active\n",
    "e = tf.multiply(e @ tf.transpose(weights['out']), tf.cast(tf.greater(fc1, 0), dtype=tf.float32)) #/ dropout\n",
    "grad_w_3 = tf.transpose(fc) @ e\n",
    "grad_b_3 = tf.reduce_sum(e, axis=0)\n",
    "\n",
    "# the last pooling layer\n",
    "e = e @ tf.transpose(weights['wd1'])\n",
    "e = tf.reshape(e, pool2.get_shape().as_list())\n",
    "\n",
    "# the last conv layer\n",
    "# unpooling get error from pooling layer\n",
    "e = func.error_pooling2conv(e, mask2)\n",
    "\n",
    "# multiply with the derivative of the active function on the conv layer\n",
    "#     this one is also important this is a part from the upsampling, but \n",
    "e = tf.multiply(e, tf.cast(tf.greater(conv2, 0), dtype=tf.float32))\n",
    "temp1, temp2 = func.filter_gradient(e, pool1, conv2)\n",
    "grad_k_2 = temp1\n",
    "grad_b_2 = temp2\n",
    "\n",
    "# conv to pool\n",
    "e = func.error_conv2pooling(e, weights['wc2'])\n",
    "\n",
    "# pool to the first conv\n",
    "e = func.error_pooling2conv(e, mask1)\n",
    "e = tf.multiply(e, tf.cast(tf.greater(conv1, 0), dtype=tf.float32))\n",
    "temp1, temp2 = func.filter_gradient(e, inputx, conv1)\n",
    "grad_k_1 = temp1\n",
    "grad_b_1 = temp2\n",
    "    \n",
    "    \n",
    "\n",
    "# gradient\n",
    "gv2 = [(grad_k_1, weights['wc1']), (grad_k_2, weights['wc2']), \n",
    "       (grad_w_3 / batch_size, weights['wd1']), (grad_w_out / batch_size, weights['out']),\n",
    "       (grad_b_1, biases['bc1']), (grad_b_2, biases['bc2']), \n",
    "       (grad_b_3 / batch_size, biases['bd1']), (grad_b_out / batch_size, biases['out'])]\n",
    "# optimizer = opt.apply_gradients(gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference between tf and mine\n",
      "0.000320435 0.000915527 0.000976562\n",
      "0.000183105 0.000213623 0.000244141\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00341797 0.00610352 0.00537109\n",
      "2.57492e-05 0.00012207 0.000106812\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000366211 0.000732422 0.000610352\n",
      "0.000183105 0.000183105 0.000221252\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00244141 0.00634766 0.00488281\n",
      "3.05176e-05 4.57764e-05 6.10352e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000488281 0.00109863 0.000976562\n",
      "0.000152588 0.00018692 0.000183105\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00244141 0.00341797 0.00439453\n",
      "2.28882e-05 6.10352e-05 6.10352e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000488281 0.000854492 0.000854492\n",
      "0.000183105 0.000244141 0.000183105\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00244141 0.00439453 0.00317383\n",
      "3.05176e-05 6.10352e-05 6.10352e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000488281 0.000854492 0.000854492\n",
      "0.000244141 0.000183105 0.000183105\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00585938 0.00415039 0.00292969\n",
      "3.05176e-05 4.57764e-05 4.57764e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000366211 0.000793457 0.000747681\n",
      "0.000183105 0.000183105 0.000213623\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00244141 0.00366211 0.00439453\n",
      "3.05176e-05 6.10352e-05 6.10352e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000366211 0.000854492 0.000976562\n",
      "0.000183105 0.000244141 0.000213623\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00341797 0.00439453 0.00488281\n",
      "3.05176e-05 0.000106812 0.000106812\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000488281 0.000732422 0.000732422\n",
      "0.000183105 0.000244141 0.000183105\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00195312 0.00439453 0.00292969\n",
      "3.05176e-05 9.15527e-05 9.15527e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000732422 0.000762939 0.000732422\n",
      "0.000183105 0.000213623 0.000183105\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00683594 0.00854492 0.00292969\n",
      "3.05176e-05 4.57764e-05 6.10352e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000488281 0.000762939 0.000732422\n",
      "0.00012207 0.000213623 0.000183105\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00292969 0.00390625 0.00341797\n",
      "4.57764e-05 7.62939e-05 9.15527e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000366211 0.000854492 0.000854492\n",
      "0.00012207 0.000183105 0.000152588\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00219727 0.00268555 0.00219727\n",
      "2.67029e-05 4.57764e-05 4.57764e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000244141 0.000732422 0.000701904\n",
      "9.15527e-05 0.000152588 0.000152588\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00219727 0.00390625 0.0020752\n",
      "2.47955e-05 6.10352e-05 6.10352e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000488281 0.000747681 0.000747681\n",
      "0.000244141 0.000213623 0.000244141\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00317383 0.00439453 0.00341797\n",
      "2.28882e-05 6.10352e-05 5.34058e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000488281 0.000854492 0.000854492\n",
      "0.000244141 0.000244141 0.000244141\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00488281 0.0078125 0.00488281\n",
      "4.57764e-05 0.00012207 7.62939e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000488281 0.00109863 0.000915527\n",
      "0.000183105 0.000259399 0.000259399\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00439453 0.00268555 0.00292969\n",
      "3.05176e-05 7.62939e-05 6.10352e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000488281 0.000854492 0.000854492\n",
      "0.00012207 0.000244141 0.000198364\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00195312 0.00585938 0.00585938\n",
      "4.57764e-05 0.000106812 0.00012207\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000366211 0.000976562 0.000976562\n",
      "0.000183105 0.000213623 0.000244141\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00268555 0.00439453 0.00244141\n",
      "3.05176e-05 6.10352e-05 6.10352e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000488281 0.000869751 0.00088501\n",
      "0.000244141 0.000183105 0.000244141\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00341797 0.00292969 0.00390625\n",
      "3.05176e-05 6.10352e-05 4.57764e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000488281 0.000915527 0.000854492\n",
      "0.00012207 0.000183105 0.000183105\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00256348 0.00366211 0.00195312\n",
      "3.43323e-05 6.10352e-05 6.10352e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000488281 0.000732422 0.000854492\n",
      "0.000183105 0.000183105 0.000244141\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00488281 0.00634766 0.00341797\n",
      "3.05176e-05 7.62939e-05 7.62939e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000488281 0.000976562 0.000854492\n",
      "0.00012207 0.000213623 0.000183105\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00561523 0.00634766 0.00341797\n",
      "3.05176e-05 6.86646e-05 6.10352e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "difference between tf and mine\n",
      "0.000488281 0.000854492 0.000793457\n",
      "0.000183105 0.000244141 0.000183105\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.00537109 0.00439453 0.00341797\n",
      "2.47955e-05 5.34058e-05 4.57764e-05\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-81fa862bc43a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Run optimization op (backprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#         ret1 all from tf.  gv1 all from mime, gv2 half half\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mret1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgv2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#         conv, pool, ee4, ee3 = sess.run([conv2, pool2, e4, e3], feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         if step % display_step == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhidou/Software/conda/envs/main/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhidou/Software/conda/envs/main/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhidou/Software/conda/envs/main/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/zhidou/Software/conda/envs/main/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhidou/Software/conda/envs/main/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# f = open('output.txt', 'w')\n",
    "# Launch the graph \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "#     while step < 2:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "#         ret1 all from tf.  gv1 all from mime, gv2 half half\n",
    "        ret1, ret2, ret3 = sess.run([gv, gv1, gv2], feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "#         conv, pool, ee4, ee3 = sess.run([conv2, pool2, e4, e3], feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "#         if step % display_step == 0:\n",
    "#             # Calculate batch loss and accuracy\n",
    "#             acc = sess.run(accuracy,feed_dict={x: batch_x,\n",
    "#                                                           y: batch_y,\n",
    "#                                                           keep_prob: 1.})\n",
    "#             print(\"Iter \" + str(step*batch_size) + \"\\nTraining Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "        print('difference between tf and mine')\n",
    "        for i, j, k in zip(ret1, ret2, ret3):\n",
    "            print(np.max(np.abs(i[0] - j[0])), np.max(np.abs(j[0] - k[0])), np.max(np.abs(i[0] - k[0])))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  236.395,    67.118,   245.784,   362.196,   363.377],\n",
       "       [  158.298,   -35.431,   132.62 ,   360.272,   310.257],\n",
       "       [  864.503,   688.634,   865.075,  1098.375,   780.748],\n",
       "       [ 1570.123,  2003.334,  1882.387,  1476.438,  1155.729],\n",
       "       [ 1494.532,  1964.872,  1653.557,  1356.253,   725.913]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret1[0][0][:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  236.395,    67.118,   245.784,   362.196,   363.377],\n",
       "       [  158.298,   -35.431,   132.619,   360.272,   310.257],\n",
       "       [  864.502,   688.634,   865.075,  1098.375,   780.748],\n",
       "       [ 1570.122,  2003.334,  1882.387,  1476.438,  1155.729],\n",
       "       [ 1494.532,  1964.872,  1653.557,  1356.253,   725.913]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret2[0][0][:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference between tf and mine\n",
      "0.060152 0.0\n",
      "0.292672 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0168953 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print('difference between tf and mine')\n",
    "for i, j in zip(ret1, ret2):\n",
    "    print(np.sum(np.abs(i[0] - j[0])), np.sum(np.abs(i[1] - j[1])))\n",
    "# print('difference between tf and half')\n",
    "# for i, k in zip(ret1, ret3):\n",
    "#     print(np.max(i[0] - k[0]), np.sum(np.abs(i[1] - k[1])))\n",
    "# print('difference between mine and half')\n",
    "# for j, k in zip(ret2, ret3):\n",
    "#     print(np.max(j[0] - k[0]), np.sum(np.abs(j[1] - k[1])))\n",
    "# print('difference between tf and tf')\n",
    "# for i, l in zip(ret1, ret4):\n",
    "#     print(np.max(i[0] - l[0]), np.sum(np.abs(i[1] - l[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as f:\n",
    "    f.write('conv2 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, conv[0,:,:,0], delimiter=', ',fmt=\"%.2f\")\n",
    "    \n",
    "with open('output.txt', 'a') as f:\n",
    "    f.write('\\npool2 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, pool[0,:,:,0], delimiter=', ',fmt=\"%.2f\")\n",
    "\n",
    "with open('output.txt', 'a') as f:\n",
    "    f.write('\\nerror4 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, ee4[0,:,:,0], delimiter=', ',fmt=\"%.2f\")\n",
    "with open('output.txt', 'a') as f:\n",
    "    f.write('\\nerror3 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, ee3[0,:,:,0], delimiter=', ',fmt=\"%.2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = np.array([range(1, 65)])\n",
    "tt = np.reshape(tt, [2, 2, 4,4], order='C')\n",
    "tt = np.transpose(tt, [0, 2,3,1])\n",
    "\n",
    "tt[0,0,3,0] = 3\n",
    "tt[0,2,0,0] = 13\n",
    "tt[0,2,1,0] = 14\n",
    "tt[0,2,3,0] = 11\n",
    "tt[0,3,2,0] = 11\n",
    "tt[0,3,3,0] = 11\n",
    "tt[0,0,0,1] = 18\n",
    "tt[0,1,0,1] = 18\n",
    "tt[0,1,1,1] = 18\n",
    "tt[0,0,2,1] = 23\n",
    "tt[0,0,3,1] = 23\n",
    "tt[0,2,0,1] = 30\n",
    "\n",
    "x = tf.constant(tt, dtype=tf.float32)\n",
    "p = func.extract_patches(x, \"VALID\", 2, 2)\n",
    "pool1, mask = func.max_pool_with_mask(p=p)\n",
    "pool2 = tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1],padding='VALID')\n",
    "\n",
    "# x = tf.reshape(x, [4,4])\n",
    "with tf.Session() as sess:\n",
    "    retx, retp, retm = sess.run([x, pool1, mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.,   8.],\n",
       "       [ 14.,  11.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retp[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0. ,  0.5,  0. ,  0.5], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retm[0,1,0,:,0]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs505]",
   "language": "python",
   "name": "conda-env-cs505-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
