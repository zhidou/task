{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import functions as func\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 300000\n",
    "batch_size = 64\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    \n",
    "    x = tf.reshape(x, shape=[64, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = func.conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Pooling (down-sampling)\n",
    "    p = func.extract_patches(conv1, 'SAME', 2, 2)\n",
    "    f = func.majority_frequency(p)\n",
    "#     maxpooling\n",
    "    maxpool = func.max_pool(p)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = func.conv2d(maxpool, weights['wc2'], biases['bc2'])\n",
    "#     Pooling (down-sampling)\n",
    "    p = func.extract_patches(conv2, 'SAME', 2, 2)\n",
    "    f = func.majority_frequency(p)\n",
    "#     maxpooling\n",
    "    maxpool = func.max_pool(p)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(maxpool, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    ofc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(ofc1, weights['out']), biases['out'])\n",
    "    return ofc1, fc1, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 14, 14, 4, 32]\n",
      "[64, 7, 7, 4, 64]\n",
      "[64, 14, 14, 64]\n"
     ]
    }
   ],
   "source": [
    "# Construct model\n",
    "# ofc1, fc1, pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "\n",
    "# ------------------------------define graph------------------------------------\n",
    "# Reshape input picture\n",
    "inputx = tf.reshape(x, shape=[batch_size, 28, 28, 1])\n",
    "# Convolution Layer\n",
    "conv1 = func.conv2d(inputx, weights['wc1'], biases['bc1'])\n",
    "# Pooling (down-sampling)\n",
    "p1 = func.extract_patches(conv1, 'SAME', 2, 2)\n",
    "f1 = func.majority_frequency(p1)\n",
    "#     maxpooling\n",
    "pool1 = func.max_pool(p1)\n",
    "\n",
    "# Convolution Layer\n",
    "conv2 = func.conv2d(pool1, weights['wc2'], biases['bc2'])\n",
    "#     Pooling (down-sampling)\n",
    "p2 = func.extract_patches(conv2, 'SAME', 2, 2)\n",
    "f2 = func.majority_frequency(p2)\n",
    "#     maxpooling\n",
    "pool2 = func.max_pool(p2)\n",
    "\n",
    "# Fully connected layer\n",
    "# Reshape conv2 output to fit fully connected layer input\n",
    "fc = tf.reshape(pool2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "fc1 = tf.add(tf.matmul(fc, weights['wd1']), biases['bd1'])\n",
    "fc1 = tf.nn.relu(fc1)\n",
    "# Apply Dropout\n",
    "ofc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "# Output, class prediction\n",
    "pred = tf.add(tf.matmul(ofc1, weights['out']), biases['out'])\n",
    "\n",
    "# ------------------------------define graph------------------------------------\n",
    "\n",
    "# -----------------------------Define loss and optimizer------------------------\n",
    "varList = [weights['wd1'], weights['out'], biases['bd1'], biases['out'], weights['wc2']]\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "gv = opt.compute_gradients(loss=cost, var_list=varList)\n",
    "\n",
    "\n",
    "# ------------------------------define gradient descent-------------------------\n",
    "\n",
    "# the last fc\n",
    "sopred = tf.nn.softmax(pred)\n",
    "e1 = sopred - y\n",
    "grad_w_out = tf.transpose(ofc1) @ e1 / batch_size\n",
    "grad_b_out = tf.reduce_sum(e1, axis=0) / batch_size\n",
    "\n",
    "# the second last fc\n",
    "drv = tf.cast(tf.greater(ofc1, 0), dtype=tf.float32)\n",
    "# we use droupout at the last second layer, then we should just update the nodes that are active\n",
    "e2 = tf.multiply(e1 @ tf.transpose(weights['out']), drv) / dropout\n",
    "grad_w_3 = tf.transpose(fc) @ e2 / batch_size\n",
    "grad_b_3 = tf.reduce_sum(e2, axis=0) / batch_size\n",
    "\n",
    "# the last pooling layer\n",
    "e3 = e2 @ tf.transpose(weights['wd1'])\n",
    "e3 = tf.reshape(e3, pool2.get_shape().as_list())\n",
    "\n",
    "# the last conv layer\n",
    "[N, H, W, K, C] = p2.get_shape().as_list()\n",
    "pool2 = tf.reshape(pool2, [N, H, W, 1, C])\n",
    "mark2 = tf.cast(tf.equal(p2, pool2), dtype=tf.float32)\n",
    "e4 = tf.multiply(mark2, tf.reshape(e3, [N, H, W, 1, C]))\n",
    "e4 = tf.reshape(e4, conv2.get_shape().as_list())\n",
    "e4 = func.extract_patches(e4, 'SAME', 2, 2)\n",
    "e4 = tf.reshape(e4, conv2.get_shape().as_list())\n",
    "e4 = tf.multiply(e4, tf.cast(tf.greater(conv2, 0), dtype=tf.float32))\n",
    "\n",
    "[N, H, W, C] = conv2.get_shape().as_list()\n",
    "print([N, H, W, C])\n",
    "ppool1 = tf.pad(pool1, tf.constant([[0,0],[2,2],[2,2],[0,0]]))\n",
    "pc1c2 = func.extract_patches(ppool1, 'VALID', H, 1)\n",
    "\n",
    "# nhwkc\n",
    "pc1c2 = tf.reshape(pc1c2, [N, 5, 5, H * H, int(C/2), 1])\n",
    "e4 = tf.reshape(e4, [N, 1, 1, H *H, 1, C])\n",
    "grad_k_2 = tf.reduce_sum(tf.multiply(pc1c2, e4), axis=3)\n",
    "grad_k_2 = tf.reduce_sum(grad_k_2, axis=0) / batch_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = opt.apply_gradients(grads_and_vars=gv)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "# Diff1, Diff2, Out = test_conv_net(x, weights, biases, keep_prob)\n",
    "# lost = []\n",
    "# for temp in Out:\n",
    "#     lost.append(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=temp, labels=y)))\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "Testing Accuracy: 0.109375\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# f = open('output.txt', 'w')\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "#     while step * batch_size < training_iters:\n",
    "    while step < 2:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "#         e = sess.run(e3, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        grad, wo, bo, w3, b3, k2 = sess.run([gv, grad_w_out, grad_b_out, grad_w_3, grad_b_3, grad_k_2], feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            cos, acc = sess.run([cost, accuracy], \n",
    "                                               feed_dict={x: batch_x,\n",
    "                                                          y: batch_y,\n",
    "                                                          keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(cos) + \n",
    "                  \"\\nTraining Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "#     Calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:64], y: mnist.test.labels[:64],\n",
    "keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(wo - grad[1][0]) - np.min(wo - grad[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(bo - grad[3][0]) - np.min(bo - grad[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(w3 - grad[0][0]) - np.min(w3 - grad[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(b3 - grad[2][0]) - np.min(b3 - grad[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.965,   7.431,  19.775,  -6.818,  -2.377],\n",
       "       [ -7.811,  27.059,   6.972,  11.021,   3.234],\n",
       "       [ 19.232,  27.129,  -6.008,  14.456,   0.253],\n",
       "       [ 19.94 ,   0.468,  -1.079,  -5.228,  -4.689],\n",
       "       [ -5.63 ,  -4.212,  -2.649, -12.436, -17.494]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad[4][0][:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.021,  13.59 ,  49.744,  -5.42 ,  15.778],\n",
       "       [ 10.47 ,  27.361,  21.727, -22.276,   0.902],\n",
       "       [ 10.571,  25.159,   5.949,   3.938,   8.808],\n",
       "       [  3.346,  18.343,  21.209,  13.723,  12.297],\n",
       "       [-18.6  ,  16.067,  22.971,  17.833,   3.507]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k2[:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = np.array([range(1, 65)])\n",
    "tt = np.reshape(tt, [2, 2, 4,4], order='C')\n",
    "tt = np.transpose(tt, [0, 2,3,1])\n",
    "\n",
    "tt[0,0,3,0] = 3\n",
    "tt[0,2,0,0] = 13\n",
    "tt[0,2,1,0] = 13\n",
    "tt[0,2,3,0] = 11\n",
    "tt[0,3,2,0] = 11\n",
    "tt[0,3,3,0] = 11\n",
    "tt[0,0,0,1] = 18\n",
    "tt[0,1,0,1] = 18\n",
    "tt[0,1,1,1] = 18\n",
    "tt[0,0,2,1] = 23\n",
    "tt[0,0,3,1] = 23\n",
    "tt[0,2,0,1] = 30\n",
    "\n",
    "x = tf.constant(tt, dtype=tf.float32)\n",
    "p = func.extract_patches(x, \"VALID\", 2, 2)\n",
    "maxx = func.max_pool(p)\n",
    "maxx = tf.reshape(maxx, [2,2,2,1,2])\n",
    "mark = tf.cast(tf.equal(p, maxx), dtype=tf.float32)\n",
    "mark = tf.multiply(mark, maxx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "p = tf.reshape(mark, x.get_shape().as_list())\n",
    "p = func.extract_patches(p, \"VALID\", 2, 2)\n",
    "p = tf.reshape(p, x.get_shape().as_list())\n",
    "\n",
    "\n",
    "# x = tf.reshape(x, [4,4])\n",
    "with tf.Session() as sess:\n",
    "    retx, retp = sess.run([mark, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt1 = np.array([[[16, 2, 3, 13], [5,11,10,8], [9,7,6,12], [4, 14,15, 1]], [[16, 2, 3, 13], [5,11,10,8], [9,7,6,12], [4, 14,15, 1]]])\n",
    "fk = np.array([[[0.8,0.1,-0.6], [0.3,0.5,0.7],[-0.4,0,-0.2]],[[0.8,0.1,-0.6], [0.3,0.5,0.7],[-0.4,0,-0.2]]])\n",
    "fk = np.array([fk, np.flip(fk,axis=1)])\n",
    "fk = fk.transpose([0,2,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 3, 2)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt1 = np.array([tt1, tt2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt1 = tt1.transpose([0,2,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16,  2,  3, 13],\n",
       "       [ 5, 11, 10,  8],\n",
       "       [ 9,  7,  6, 12],\n",
       "       [ 4, 14, 15,  1]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt1[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 9, 2]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(tt1,dtype=tf.float32)\n",
    "y = tf.constant(fk, dtype=tf.float32)\n",
    "y = tf.reshape(y, [2,1,1,9,1,2])\n",
    "px = func.extract_patches(x, ksize=3, padding='VALID',stride=1)\n",
    "print(px.get_shape().as_list())\n",
    "px = tf.reshape(px, [2,2,2,9,2,1])\n",
    "temp1 = tf.multiply(px, y)\n",
    "temp1 = tf.reduce_sum(temp1, axis=3)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ret = sess.run(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2, 2, 2)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.],\n",
       "       [  0.,   6.,   0.,   8.],\n",
       "       [  0.,   0.,  11.,  11.],\n",
       "       [  0.,  14.,  11.,  11.]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retp[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retx[0,0,0,:,0]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
