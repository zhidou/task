{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 300000\n",
    "batch_size = 128\n",
    "mini_batch = 16\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------define graph------------------------------------\n",
    "# Reshape input picture\n",
    "inputxs = tf.split(tf.reshape(x, shape=[batch_size, 28, 28, 1]), axis=0, num_or_size_splits=int(batch_size/mini_batch))\n",
    "ys = tf.split(y, axis=0, num_or_size_splits=int(batch_size/mini_batch))\n",
    "grad_k_1 = tf.zeros_like(weights['wc1'])\n",
    "grad_k_2 = tf.zeros_like(weights['wc2'])\n",
    "grad_w_3 = tf.zeros_like(weights['wd1'])\n",
    "grad_w_out = tf.zeros_like(weights['out'])\n",
    "grad_b_1 = tf.zeros_like(biases['bc1'])\n",
    "grad_b_2 = tf.zeros_like(biases['bc2'])\n",
    "grad_b_3 = tf.zeros_like(biases['bd1'])\n",
    "grad_b_out = tf.zeros_like(biases['out'])\n",
    "correct_pred = tf.zeros(mini_batch)\n",
    "\n",
    "for inputx, yi in zip(inputxs, ys):\n",
    "    # Convolution Layer\n",
    "    conv1 = func.conv2d(inputx, weights['wc1'], biases['bc1'])\n",
    "    # Pooling (down-sampling)\n",
    "    p1 = func.extract_patches(conv1, 'SAME', 2, 2)\n",
    "    f1 = func.majority_frequency(p1)\n",
    "    #     maxpooling\n",
    "    pool1, mask1 = func.max_pool_with_mask(p1)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = func.conv2d(pool1, weights['wc2'], biases['bc2'])\n",
    "    #     Pooling (down-sampling)\n",
    "    p2 = func.extract_patches(conv2, 'SAME', 2, 2)\n",
    "    f2 = func.majority_frequency(p2)\n",
    "    #     maxpooling\n",
    "    pool2, mask2 = func.max_pool_with_mask(p2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc = tf.reshape(pool2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    pred = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    correct_pred += tf.cast(tf.equal(tf.argmax(pred, 1), tf.argmax(yi, 1)), dtype=tf.float32)\n",
    "    \n",
    "    # ------------------------------define graph------------------------------------\n",
    "\n",
    "    # -----------------------------Define loss and optimizer------------------------\n",
    "    # varList = [weights['wd1'], weights['out'], biases['bd1'], biases['out'], weights['wc2'], biases['bc2'], weights['wc1'], biases['bc1']]\n",
    "    # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    # opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    # gv = opt.compute_gradients(loss=cost)\n",
    "\n",
    "\n",
    "    # ------------------------------define gradient descent-------------------------\n",
    "\n",
    "    # the last fc\n",
    "    e = tf.nn.softmax(pred) - yi\n",
    "    grad_w_out += tf.transpose(fc1) @ e\n",
    "    grad_b_out += tf.reduce_sum(e, axis=0)\n",
    "\n",
    "    # the second last fc\n",
    "    # we use droupout at the last second layer, then we should just update the nodes that are active\n",
    "    e = tf.multiply(e @ tf.transpose(weights['out']), tf.cast(tf.greater(fc1, 0), dtype=tf.float32)) / dropout\n",
    "    grad_w_3 += tf.transpose(fc) @ e\n",
    "    grad_b_3 += tf.reduce_sum(e, axis=0)\n",
    "\n",
    "    # the last pooling layer\n",
    "    e = e @ tf.transpose(weights['wd1'])\n",
    "    e = tf.reshape(e, pool2.get_shape().as_list())\n",
    "\n",
    "    # the last conv layer\n",
    "    # unpooling get error from pooling layer\n",
    "    e = func.error_pooling2conv(e, mask2)\n",
    "\n",
    "    # multiply with the derivative of the active function on the conv layer\n",
    "#     this one is also important this is a part from the upsampling, but \n",
    "    e = tf.multiply(e, tf.cast(tf.greater(conv2, 0), dtype=tf.float32))\n",
    "    temp1, temp2 = func.filter_gradient(e, pool1, conv2)\n",
    "    grad_k_2 += temp1\n",
    "    grad_b_2 += temp2\n",
    "\n",
    "    # conv to pool\n",
    "    e = func.error_conv2pooling(e, weights['wc2'])\n",
    "\n",
    "    # pool to the first conv\n",
    "    e = func.error_pooling2conv(e, mask1)\n",
    "    e = tf.multiply(e, tf.cast(tf.greater(conv1, 0), dtype=tf.float32))\n",
    "    temp1, temp2 = func.filter_gradient(e, inputx, conv1)\n",
    "    grad_k_1 += temp1\n",
    "    grad_b_1 += temp2\n",
    "    del conv1, conv2, p1, f1, p2, f3, pool1, pool2, mask1, mask2, fc1, fc, e\n",
    "    \n",
    "    \n",
    "\n",
    "# gradient\n",
    "gv1 = [(grad_k_1 / batch_size, weights['wc1']), (grad_k_2 / batch_size, weights['wc2']), \n",
    "       (grad_w_3 / batch_size, weights['wd1']), (grad_w_out / batch_size, weights['out']),\n",
    "       (grad_b_1 / batch_size, biases['bc1']), (grad_b_2/ batch_size, biases['bc2']), \n",
    "       (grad_b_3 / batch_size, biases['bd1']), (grad_b_out / batch_size, biases['out'])]\n",
    "\n",
    "\n",
    "# apply gradient\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimize = opt.apply_gradients(gv1)\n",
    "\n",
    "# optimizer = opt.apply_gradients(grads_and_vars=gv)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "# Diff1, Diff2, Out = test_conv_net(x, weights, biases, keep_prob)\n",
    "# lost = []\n",
    "# for temp in Out:\n",
    "#     lost.append(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=temp, labels=y)))\n",
    "\n",
    "# correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(correct_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8125\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# f = open('output.txt', 'w')\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "#     while step * batch_size < training_iters:\n",
    "    while step < 2:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimize, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            acc = sess.run(accuracy,feed_dict={x: batch_x,\n",
    "                                                          y: batch_y,\n",
    "                                                          keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) + \"\\nTraining Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "#     Calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:batch_size], y: mnist.test.labels[:batch_size],\n",
    "keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as f:\n",
    "    f.write('conv2 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, pa2[0,:,:,0], delimiter=', ',fmt=\"%.2f\")\n",
    "    \n",
    "with open('output.txt', 'a') as f:\n",
    "    f.write('\\npool2 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, po2[0,:,:,0], delimiter=', ',fmt=\"%.2f\")\n",
    "\n",
    "with open('output.txt', 'a') as f:\n",
    "    f.write('\\nerror3 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, ee3[0,:,:,0], delimiter=', ',fmt=\"%.2f\")\n",
    "\n",
    "with open('output.txt', 'a') as f:\n",
    "    f.write('\\nfirst pool value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, po1[0,:,:,0], delimiter=', ',fmt=\"%.2f\")\n",
    "    \n",
    "with open('output.txt', 'a') as f:\n",
    "    f.write('\\nerror4 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, ee4[0,:,:,0], delimiter=', ',fmt=\"%.2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 784)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0012207 0.0\n",
      "0.000549316 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.00622559 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "for g, g1 in zip(grad, grad1):\n",
    "    print(np.max(g[0] - g1[0]) - np.min(g[0] - g1[0]), np.max(g[1] - g1[1]) - np.min(g[1] - g1[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.622,  0.622,\n",
       "        0.622,  0.622,  0.622,  0.622,  0.622,  0.622,  0.622,  0.622,\n",
       "        0.622,  0.622,  0.   ,  0.   ,  0.622,  0.622,  0.622,  0.622,\n",
       "        0.622,  0.622,  0.622,  0.622,  0.622,  0.622,  0.622,  0.622,\n",
       "        0.   ,  0.   ,  0.622,  0.622,  0.622,  2.578,  2.995,  2.898,\n",
       "        2.34 ,  3.241,  2.774,  3.216,  1.88 ,  0.   ,  0.   ,  0.   ,\n",
       "        0.622,  0.622,  0.622,  4.386,  3.872,  3.893,  3.087,  2.465,\n",
       "        2.141,  2.869,  2.074,  0.   ,  0.   ,  0.   ,  0.622,  0.622,\n",
       "        0.622,  2.077,  0.737,  0.031,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ,  0.622,  0.622,  0.622,  0.447,\n",
       "        0.   ,  0.   ,  0.   ,  0.75 ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.622,  0.622,  0.622,  0.622,  0.622,  1.976,\n",
       "        4.029,  3.67 ,  0.178,  0.   ,  0.   ,  0.205,  0.   ,  0.   ,\n",
       "        0.622,  0.622,  0.622,  0.622,  2.304,  4.501,  3.834,  0.703,\n",
       "        0.   ,  0.   ,  0.   ,  0.622,  0.   ,  0.   ,  0.622,  0.622,\n",
       "        0.622,  2.027,  4.828,  3.574,  0.698,  0.   ,  0.   ,  0.   ,\n",
       "        0.622,  0.622,  0.   ,  0.   ,  0.622,  0.622,  0.813,  4.458,\n",
       "        3.953,  0.766,  0.   ,  0.   ,  0.   ,  0.622,  0.622,  0.622,\n",
       "        0.   ,  0.   ,  0.622,  0.622,  4.076,  4.693,  1.181,  0.   ,\n",
       "        0.   ,  0.   ,  0.622,  0.622,  0.622,  0.622,  0.   ,  0.   ,\n",
       "        0.622,  0.622,  4.835,  1.674,  0.314,  0.   ,  0.   ,  0.594,\n",
       "        0.622,  0.622,  0.622,  0.622], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc1[0,0,0,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 14, 14, 64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(wo - grad[1][0]) - np.min(wo - grad[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(bo - grad[3][0]) - np.min(bo - grad[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(w3 - grad[0][0]) - np.min(w3 - grad[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.9406967e-08"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(b3 - grad[2][0]) - np.min(b3 - grad[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00045776367"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(k2 - grad[4][0]) - np.min(k2 - grad[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.9182129e-05"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(b2 - grad[5][0]) - np.min(b2 - grad[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -315.516,  -274.134,  -110.675,   214.852,   469.886],\n",
       "       [ -139.565,  -354.963,   -14.112,   389.47 ,   461.621],\n",
       "       [ -124.195,  -157.435,   383.971,   661.055,   526.735],\n",
       "       [  419.112,   513.28 ,   890.624,  1040.974,  1066.86 ],\n",
       "       [  804.38 ,   680.623,   922.307,  1160.497,  1113.75 ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad[6][0][:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -315.516,  -274.134,  -110.675,   214.852,   469.886],\n",
       "       [ -139.565,  -354.963,   -14.112,   389.47 ,   461.621],\n",
       "       [ -124.195,  -157.435,   383.97 ,   661.055,   526.735],\n",
       "       [  419.112,   513.28 ,   890.624,  1040.974,  1066.86 ],\n",
       "       [  804.38 ,   680.623,   922.307,  1160.497,  1113.75 ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1[:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -208.329,  1371.998,  2677.15 , -1897.6  ,  1936.688,    44.462,\n",
       "        -160.821,   742.401,   621.907,  2912.838,  1529.066,  3433.003,\n",
       "         135.857, -1745.254,  2003.65 ,    17.914,  3355.716,  -864.316,\n",
       "         725.766,   287.84 ,  -320.119,   391.264,  2012.347,  4179.506,\n",
       "        -473.446,  1613.412,   398.612,  3576.725,  3137.878,  2353.416,\n",
       "        2589.489,   423.992], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -208.329,  1372.001,  2677.152, -1897.599,  1936.688,    44.462,\n",
       "        -160.821,   742.401,   621.907,  2912.84 ,  1529.065,  3433.004,\n",
       "         135.857, -1745.253,  2003.649,    17.914,  3355.718,  -864.316,\n",
       "         725.767,   287.84 ,  -320.119,   391.264,  2012.346,  4179.504,\n",
       "        -473.446,  1613.411,   398.612,  3576.727,  3137.877,  2353.414,\n",
       "        2589.489,   423.992], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  10.479,   40.462,  -20.684,  -68.879,  -72.81 ],\n",
       "       [   7.344,    1.926,  -51.897,  -53.197,  -39.338],\n",
       "       [  49.751,   -7.294,   -6.286,  -43.237,  -71.886],\n",
       "       [  -6.139,   14.189,  -37.93 , -101.595,  -38.394],\n",
       "       [ -29.756,  -19.656,  -55.514,  -51.635,    2.628]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k2[:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -18.696,    9.353,  -38.062,   10.527,  -15.099,  -22.246,\n",
       "        120.452,   56.936,  -12.423,  -74.125,   60.832,   52.296,\n",
       "         46.787,  -70.468,   -1.294,   55.804,   45.347,  -25.526,\n",
       "         59.663,   24.105,    7.68 ,   33.834,  -59.61 ,   34.805,\n",
       "        -28.622,   26.149,  -25.254,  -46.559,   70.685,   54.573,\n",
       "         -9.246,   26.074,   90.612,   34.13 ,  -12.593,   26.451,\n",
       "         44.761,  119.004,   33.882,  -55.635,   -5.529,   11.017,\n",
       "         13.544,   99.017,   82.256,  -29.706,  -50.963,  -51.528,\n",
       "         -6.476,   24.699,  111.793,   48.072,  109.696,   67.398,\n",
       "          3.102,    1.572,  -20.94 ,   20.755,    4.561,   11.462,\n",
       "         69.632,   -2.096,  -36.653,   53.967], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -18.696,    9.353,  -38.062,   10.527,  -15.099,  -22.246,\n",
       "        120.452,   56.936,  -12.423,  -74.125,   60.832,   52.296,\n",
       "         46.787,  -70.468,   -1.294,   55.804,   45.347,  -25.526,\n",
       "         59.663,   24.105,    7.68 ,   33.834,  -59.61 ,   34.805,\n",
       "        -28.622,   26.149,  -25.254,  -46.559,   70.685,   54.573,\n",
       "         -9.246,   26.074,   90.612,   34.13 ,  -12.593,   26.451,\n",
       "         44.761,  119.004,   33.882,  -55.635,   -5.529,   11.017,\n",
       "         13.544,   99.017,   82.256,  -29.706,  -50.963,  -51.528,\n",
       "         -6.476,   24.699,  111.793,   48.072,  109.696,   67.398,\n",
       "          3.102,    1.573,  -20.94 ,   20.755,    4.561,   11.462,\n",
       "         69.632,   -2.096,  -36.653,   53.967], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = np.array([range(1, 65)])\n",
    "tt = np.reshape(tt, [2, 2, 4,4], order='C')\n",
    "tt = np.transpose(tt, [0, 2,3,1])\n",
    "\n",
    "tt[0,0,3,0] = 3\n",
    "tt[0,2,0,0] = 13\n",
    "tt[0,2,1,0] = 13\n",
    "tt[0,2,3,0] = 11\n",
    "tt[0,3,2,0] = 11\n",
    "tt[0,3,3,0] = 11\n",
    "tt[0,0,0,1] = 18\n",
    "tt[0,1,0,1] = 18\n",
    "tt[0,1,1,1] = 18\n",
    "tt[0,0,2,1] = 23\n",
    "tt[0,0,3,1] = 23\n",
    "tt[0,2,0,1] = 30\n",
    "\n",
    "x = tf.constant(tt, dtype=tf.float32)\n",
    "p = func.extract_patches(x, \"VALID\", 2, 2)\n",
    "maxx = func.max_pool(p)\n",
    "maxx = tf.reshape(maxx, [2,2,2,1,2])\n",
    "mark = tf.cast(tf.equal(p, maxx), dtype=tf.float32)\n",
    "mark = tf.multiply(mark, maxx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "p = tf.reshape(mark, x.get_shape().as_list())\n",
    "p = func.extract_patches(p, \"VALID\", 2, 2)\n",
    "p = tf.reshape(p, x.get_shape().as_list())\n",
    "\n",
    "\n",
    "# x = tf.reshape(x, [4,4])\n",
    "with tf.Session() as sess:\n",
    "    retx, retp = sess.run([mark, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fk = np.array([\n",
    "    [\n",
    "        [0.8,0.1,-0.6], [0.3,0.5,0.7],[-0.4,0,-0.2]\n",
    "    ],\n",
    "    [\n",
    "        [0.8,0.1,-0.6], [0.3,0.5,0.7],[-0.4,0,-0.2]\n",
    "    ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt1 = np.array([\n",
    "    [\n",
    "        [16, 2, 3, 13], [5,11,10,8], [9,7,6,12], [4, 14,15, 1]\n",
    "    ], \n",
    "    [\n",
    "        [16, 2, 3, 13], [5,11,10,8], [9,7,6,12], [4, 14,15, 1]\n",
    "    ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt1 = np.array([[[16, 2, 3, 13], [5,11,10,8], [9,7,6,12], [4, 14,15, 1]], [[16, 2, 3, 13], [5,11,10,8], [9,7,6,12], [4, 14,15, 1]]])\n",
    "tt1 = np.array([tt1, np.flip(tt1, axis=1)])\n",
    "tt1 = tt1.transpose([0,2,3,1])\n",
    "fk = np.array([[[0.8,0.1,-0.], [0.3,0.5,0.7],[-0.4,0,-0.2]]])\n",
    "fk = np.array([fk, np.flip(fk,axis=1)])\n",
    "fk = fk.transpose([0,2,3,1])\n",
    "fk = fk.transpose([1,2,3,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 4, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 1, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 14, 15,  1],\n",
       "       [ 9,  7,  6, 12],\n",
       "       [ 5, 11, 10,  8],\n",
       "       [16,  2,  3, 13]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt1[1,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xw = tf.constant(tt1, dtype=tf.float32)\n",
    "yw = tf.zeros(10)\n",
    "# xw = tf.split(xw, axis=0, num_or_size_splits=2)\n",
    "# for i in xw:\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    retx = sess.run(yw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'zeros_like_8:0' shape=(2, 1, 4, 4, 2) dtype=float32>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_8:0' shape=(2, 4, 4, 2) dtype=float32>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  2.  2.  2.]\n",
      " [ 2.  2.  2.  2.]\n",
      " [ 2.  2.  2.  2.]\n",
      " [ 2.  2.  2.  2.]]\n"
     ]
    }
   ],
   "source": [
    "print(retx[1,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor 'gradients_5/MatMul_10_grad/tuple/control_dependency_1:0' shape=(3136, 1024) dtype=float32>,\n",
       "  <tf.Variable 'Variable_10:0' shape=(3136, 1024) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_5/MatMul_11_grad/tuple/control_dependency_1:0' shape=(1024, 10) dtype=float32>,\n",
       "  <tf.Variable 'Variable_11:0' shape=(1024, 10) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_5/Add_10_grad/tuple/control_dependency_1:0' shape=(1024,) dtype=float32>,\n",
       "  <tf.Variable 'Variable_14:0' shape=(1024,) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_5/Add_11_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>,\n",
       "  <tf.Variable 'Variable_15:0' shape=(10,) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_5/Conv2D_16_grad/tuple/control_dependency_1:0' shape=(5, 5, 32, 64) dtype=float32>,\n",
       "  <tf.Variable 'Variable_9:0' shape=(5, 5, 32, 64) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_5/BiasAdd_11_grad/tuple/control_dependency_1:0' shape=(64,) dtype=float32>,\n",
       "  <tf.Variable 'Variable_13:0' shape=(64,) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_5/Conv2D_15_grad/tuple/control_dependency_1:0' shape=(5, 5, 1, 32) dtype=float32>,\n",
       "  <tf.Variable 'Variable_8:0' shape=(5, 5, 1, 32) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_5/BiasAdd_10_grad/tuple/control_dependency_1:0' shape=(32,) dtype=float32>,\n",
       "  <tf.Variable 'Variable_12:0' shape=(32,) dtype=float32_ref>)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.ops.variables.Variable"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gv[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to literal (<ipython-input-60-1f5fa26a16a9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-60-1f5fa26a16a9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    2,3 += 3\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to literal\n"
     ]
    }
   ],
   "source": [
    "2,3 += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 3\n",
    "del a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3b5d5c371295>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "b"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
