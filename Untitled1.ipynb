{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import functions as func\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 300000\n",
    "batch_size = 64\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    \n",
    "    x = tf.reshape(x, shape=[64, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = func.conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Pooling (down-sampling)\n",
    "    p = func.extract_patches(conv1, 'SAME', 2, 2)\n",
    "    f = func.majority_frequency(p)\n",
    "#     maxpooling\n",
    "    maxpool = func.max_pool(p)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = func.conv2d(maxpool, weights['wc2'], biases['bc2'])\n",
    "#     Pooling (down-sampling)\n",
    "    p = func.extract_patches(conv2, 'SAME', 2, 2)\n",
    "    f = func.majority_frequency(p)\n",
    "#     maxpooling\n",
    "    maxpool = func.max_pool(p)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(maxpool, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    ofc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(ofc1, weights['out']), biases['out'])\n",
    "    return ofc1, fc1, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 14, 14, 4, 32]\n",
      "[64, 7, 7, 4, 64]\n",
      "[64, 14, 14, 64]\n"
     ]
    }
   ],
   "source": [
    "# Construct model\n",
    "# ofc1, fc1, pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "\n",
    "# ------------------------------define graph------------------------------------\n",
    "# Reshape input picture\n",
    "inputx = tf.reshape(x, shape=[batch_size, 28, 28, 1])\n",
    "# Convolution Layer\n",
    "conv1 = func.conv2d(inputx, weights['wc1'], biases['bc1'])\n",
    "# Pooling (down-sampling)\n",
    "p1 = func.extract_patches(conv1, 'SAME', 2, 2)\n",
    "f1 = func.majority_frequency(p1)\n",
    "#     maxpooling\n",
    "pool1 = func.max_pool(p1)\n",
    "\n",
    "# Convolution Layer\n",
    "conv2 = func.conv2d(pool1, weights['wc2'], biases['bc2'])\n",
    "#     Pooling (down-sampling)\n",
    "p2 = func.extract_patches(conv2, 'SAME', 2, 2)\n",
    "f2 = func.majority_frequency(p2)\n",
    "#     maxpooling\n",
    "pool2 = func.max_pool(p2)\n",
    "\n",
    "# Fully connected layer\n",
    "# Reshape conv2 output to fit fully connected layer input\n",
    "fc = tf.reshape(pool2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "fc1 = tf.add(tf.matmul(fc, weights['wd1']), biases['bd1'])\n",
    "fc1 = tf.nn.relu(fc1)\n",
    "# Apply Dropout\n",
    "ofc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "# Output, class prediction\n",
    "pred = tf.add(tf.matmul(ofc1, weights['out']), biases['out'])\n",
    "\n",
    "# ------------------------------define graph------------------------------------\n",
    "\n",
    "# -----------------------------Define loss and optimizer------------------------\n",
    "varList = [weights['wd1'], weights['out'], biases['bd1'], biases['out'], weights['wc2'], biases['bc2']]\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "gv = opt.compute_gradients(loss=cost, var_list=varList)\n",
    "\n",
    "\n",
    "# ------------------------------define gradient descent-------------------------\n",
    "\n",
    "# the last fc\n",
    "sopred = tf.nn.softmax(pred)\n",
    "e1 = sopred - y\n",
    "grad_w_out = tf.transpose(ofc1) @ e1 / batch_size\n",
    "grad_b_out = tf.reduce_sum(e1, axis=0) / batch_size\n",
    "\n",
    "# the second last fc\n",
    "drv = tf.cast(tf.greater(ofc1, 0), dtype=tf.float32)\n",
    "# we use droupout at the last second layer, then we should just update the nodes that are active\n",
    "e2 = tf.multiply(e1 @ tf.transpose(weights['out']), drv) / dropout\n",
    "grad_w_3 = tf.transpose(fc) @ e2 / batch_size\n",
    "grad_b_3 = tf.reduce_sum(e2, axis=0) / batch_size\n",
    "\n",
    "# the last pooling layer\n",
    "e3 = e2 @ tf.transpose(weights['wd1'])\n",
    "e3 = tf.reshape(e3, pool2.get_shape().as_list())\n",
    "\n",
    "# the last conv layer\n",
    "[N, H, W, K, C] = p2.get_shape().as_list()\n",
    "ppool2 = tf.reshape(pool2, [N, H, W, 1, C])\n",
    "mark2 = tf.cast(tf.equal(p2, ppool2), dtype=tf.float32)\n",
    "e4 = tf.multiply(mark2, tf.reshape(e3, [N, H, W, 1, C]))\n",
    "e4 = tf.reshape(e4, conv2.get_shape().as_list())\n",
    "\n",
    "e4 = func.extract_patches(e4, 'SAME', 2, 2)\n",
    "e4 = tf.reshape(e4, conv2.get_shape().as_list())\n",
    "e4 = tf.multiply(e4, tf.cast(tf.greater(conv2, 0), dtype=tf.float32))\n",
    "\n",
    "[N, H, W, C] = conv2.get_shape().as_list()\n",
    "print([N, H, W, C])\n",
    "ppool1 = tf.pad(pool1, tf.constant([[0,0],[2,2],[2,2],[0,0]]))\n",
    "pc1c2 = func.extract_patches(ppool1, 'VALID', H, 1)\n",
    "\n",
    "# nhwkc\n",
    "pc1c2 = tf.reshape(pc1c2, [N, 5, 5, H * H, int(C/2), 1])\n",
    "grad_k_2 = tf.reduce_sum(tf.multiply(pc1c2, tf.reshape(e4, [N, 1, 1, H *H, 1, C])), axis=3)\n",
    "grad_k_2 = tf.reduce_sum(grad_k_2, axis=0) / batch_size\n",
    "grad_b_2 = tf.reduce_sum(e4, axis=[0,1,2]) / batch_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = opt.apply_gradients(grads_and_vars=gv)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "# Diff1, Diff2, Out = test_conv_net(x, weights, biases, keep_prob)\n",
    "# lost = []\n",
    "# for temp in Out:\n",
    "#     lost.append(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=temp, labels=y)))\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "Testing Accuracy: 0.03125\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# f = open('output.txt', 'w')\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "#     while step * batch_size < training_iters:\n",
    "    while step < 2:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "#         e = sess.run(e3, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        grad, wo, bo, w3, b3, k2, b2 = sess.run([gv, grad_w_out, grad_b_out, grad_w_3, grad_b_3, grad_k_2, grad_b_2], feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            cos, acc = sess.run([cost, accuracy], \n",
    "                                               feed_dict={x: batch_x,\n",
    "                                                          y: batch_y,\n",
    "                                                          keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(cos) + \n",
    "                  \"\\nTraining Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "#     Calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:64], y: mnist.test.labels[:64],\n",
    "keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(wo - grad[1][0]) - np.min(wo - grad[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(bo - grad[3][0]) - np.min(bo - grad[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(w3 - grad[0][0]) - np.min(w3 - grad[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(b3 - grad[2][0]) - np.min(b3 - grad[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.113,   3.941,   1.01 ,   1.72 ,   1.329],\n",
       "       [  3.869,   4.16 ,   6.256,   3.623,   2.559],\n",
       "       [ -0.946,  -0.315,   7.511,  10.412,   5.467],\n",
       "       [  2.677,  -0.286,   3.114,   4.576,  -3.163],\n",
       "       [  1.466,  -4.366,  -6.594,  -1.089,  -0.11 ]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad[4][0][:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-12.055,  -9.243,  -2.044,  -0.433,  -1.15 ],\n",
       "       [ -3.359,  -8.579, -10.623,  -0.646,   8.339],\n",
       "       [  0.018,  -0.573, -10.659,  -6.346,   2.788],\n",
       "       [ -4.544,  -3.988, -10.051,  -9.829,  -5.183],\n",
       "       [  2.72 ,   8.593,  12.997,   6.47 ,  -2.174]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k2[:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   8.486,   32.147,   32.594,   18.176,   19.368,   37.689,\n",
       "          3.461,    3.161,   36.679,   46.331,   35.181,  -17.983,\n",
       "        -26.24 ,  -17.523,   22.676,   11.221,  -15.314,  -27.324,\n",
       "         18.39 ,   -6.823, -112.775,  -39.195,   69.488,   29.929,\n",
       "        -52.463,  -19.913,   38.214,    3.871, -108.608,   31.039,\n",
       "        -11.308,   12.716,   -5.73 ,  142.202,   64.528,    9.899,\n",
       "          4.524,   76.403,   17.972,    1.867,   52.849,  -16.054,\n",
       "        145.009,  -36.019,  -51.141,   44.265,   89.041,   24.233,\n",
       "         40.731,   24.423,   47.278,   49.88 ,   28.943,   31.006,\n",
       "         19.901,   19.968,   -1.874,  -36.756,   37.426,   85.944,\n",
       "        -13.443,   36.236,   -7.993,  121.819], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.033,  0.13 ,  0.136,  0.29 , -0.019,  0.231,  0.074,  0.022,\n",
       "        0.205,  0.094,  0.188, -0.427, -0.289, -0.115,  0.145, -0.141,\n",
       "       -0.035, -0.067, -0.072, -0.23 , -0.203, -0.269,  0.299,  0.296,\n",
       "       -0.4  ,  0.369, -0.033, -0.292, -0.125, -0.183, -0.156,  0.02 ,\n",
       "       -0.125,  0.793,  0.4  , -0.108, -0.005,  0.189, -0.643,  0.012,\n",
       "        0.222, -0.317,  0.744, -0.026, -0.279, -0.004,  0.386, -0.056,\n",
       "        0.131, -0.234,  0.081,  0.156,  0.253,  0.409, -0.04 ,  0.154,\n",
       "        0.055, -0.391,  0.101,  0.962, -0.03 , -0.251,  0.186,  0.539], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = np.array([range(1, 65)])\n",
    "tt = np.reshape(tt, [2, 2, 4,4], order='C')\n",
    "tt = np.transpose(tt, [0, 2,3,1])\n",
    "\n",
    "tt[0,0,3,0] = 3\n",
    "tt[0,2,0,0] = 13\n",
    "tt[0,2,1,0] = 13\n",
    "tt[0,2,3,0] = 11\n",
    "tt[0,3,2,0] = 11\n",
    "tt[0,3,3,0] = 11\n",
    "tt[0,0,0,1] = 18\n",
    "tt[0,1,0,1] = 18\n",
    "tt[0,1,1,1] = 18\n",
    "tt[0,0,2,1] = 23\n",
    "tt[0,0,3,1] = 23\n",
    "tt[0,2,0,1] = 30\n",
    "\n",
    "x = tf.constant(tt, dtype=tf.float32)\n",
    "p = func.extract_patches(x, \"VALID\", 2, 2)\n",
    "maxx = func.max_pool(p)\n",
    "maxx = tf.reshape(maxx, [2,2,2,1,2])\n",
    "mark = tf.cast(tf.equal(p, maxx), dtype=tf.float32)\n",
    "mark = tf.multiply(mark, maxx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "p = tf.reshape(mark, x.get_shape().as_list())\n",
    "p = func.extract_patches(p, \"VALID\", 2, 2)\n",
    "p = tf.reshape(p, x.get_shape().as_list())\n",
    "\n",
    "\n",
    "# x = tf.reshape(x, [4,4])\n",
    "with tf.Session() as sess:\n",
    "    retx, retp = sess.run([mark, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt1 = np.array([[[16, 2, 3, 13], [5,11,10,8], [9,7,6,12], [4, 14,15, 1]], [[16, 2, 3, 13], [5,11,10,8], [9,7,6,12], [4, 14,15, 1]]])\n",
    "fk = np.array([[[0.8,0.1,-0.6], [0.3,0.5,0.7],[-0.4,0,-0.2]],[[0.8,0.1,-0.6], [0.3,0.5,0.7],[-0.4,0,-0.2]]])\n",
    "fk = np.array([fk, np.flip(fk,axis=1)])\n",
    "fk = fk.transpose([0,2,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 3, 2)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt1 = np.array([tt1, tt2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt1 = tt1.transpose([0,2,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16,  2,  3, 13],\n",
       "       [ 5, 11, 10,  8],\n",
       "       [ 9,  7,  6, 12],\n",
       "       [ 4, 14, 15,  1]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt1[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.constant(np.array([[[1],[2],[3]],[[3],[4],[5]]]))\n",
    "y = tf.constant(np.array([[[2],[4]], [[3], [6]]]))\n",
    "y = tf.reshape(y, [2,1,1,2])\n",
    "x = tf.reshape(x, [2,1,3,1])\n",
    "temp1 = tf.multiply(x, y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ret = sess.run(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2,  4],\n",
       "        [ 4,  8],\n",
       "        [ 6, 12]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.],\n",
       "       [  0.,   6.,   0.,   8.],\n",
       "       [  0.,   0.,  11.,  11.],\n",
       "       [  0.,  14.,  11.,  11.]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retp[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retx[0,0,0,:,0]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
