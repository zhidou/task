{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "mini_batch = 16\n",
    "train_iter = batch_size//mini_batch\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "para = [weights['wc1'], weights['wc2'], weights['wd1'], weights['out'], biases['bc1'], biases['bc2'], biases['bd1'], biases['out']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body2():\n",
    "    # Convolution Layer\n",
    "    inputx = x.read(index=i)\n",
    "    conv1 = func.conv2d(inputx, weights['wc1'], biases['bc1'])\n",
    "    # Pooling (down-sampling)\n",
    "    p1 = func.extract_patches(conv1, 'SAME', 2, 2)\n",
    "    f1 = func.majority_frequency(p1)\n",
    "    #     maxpooling\n",
    "    pool1, mask1 = func.pca_pool_with_mask(temp=p1)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = func.conv2d(pool1, weights['wc2'], biases['bc2'])\n",
    "    #     Pooling (down-sampling)\n",
    "    p2 = func.extract_patches(conv2, 'SAME', 2, 2)\n",
    "    f2 = func.majority_frequency(p2)\n",
    "    #     maxpooling\n",
    "    pool2, mask2 = func.pca_pool_with_mask(temp=p2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc = tf.reshape(pool2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    yi = y.read(index=i)\n",
    "    pred = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    temp_correct_pred = tf.cast(tf.equal(tf.argmax(pred, 1), tf.argmax(yi, 1)), dtype=tf.float32)\n",
    "    grads[8] = grads[8].write(index=i, value=temp_correct_pred)\n",
    "\n",
    "    # ------------------------------define graph------------------------------------\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    gv = opt.compute_gradients(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)))\n",
    "    for j in range(8):\n",
    "        grads[j] = grads[j].write(index=i, value=gv[j][0])\n",
    "    \n",
    "    \n",
    "    del conv1, conv2, p1, f1, p2, f2, pool1, pool2, mask1, mask2, fc1, fc, e\n",
    "    return i, x, y, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def body(i, x, y, grads):\n",
    "    # Convolution Layer\n",
    "    inputx = x.read(index=i)\n",
    "    conv1 = func.conv2d(inputx, weights['wc1'], biases['bc1'])\n",
    "    # Pooling (down-sampling)\n",
    "    p1 = func.extract_patches(conv1, 'SAME', 2, 2)\n",
    "    f1 = func.majority_frequency(p1)\n",
    "    #     maxpooling\n",
    "    pool1, mask1 = func.pca_pool_with_mask(temp=p1)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = func.conv2d(pool1, weights['wc2'], biases['bc2'])\n",
    "    #     Pooling (down-sampling)\n",
    "    p2 = func.extract_patches(conv2, 'SAME', 2, 2)\n",
    "    f2 = func.majority_frequency(p2)\n",
    "    #     maxpooling\n",
    "    pool2, mask2 = func.pca_pool_with_mask(temp=p2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc = tf.reshape(pool2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    yi = y.read(index=i)\n",
    "    pred = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    temp_correct_pred = tf.cast(tf.equal(tf.argmax(pred, 1), tf.argmax(yi, 1)), dtype=tf.float32)\n",
    "    grads[8] = grads[8].write(index=i, value=temp_correct_pred)\n",
    "\n",
    "    # ------------------------------define graph------------------------------------\n",
    "\n",
    "    # ------------------------------define gradient descent-------------------------\n",
    "\n",
    "    # the last fc\n",
    "    e = tf.nn.softmax(pred) - yi\n",
    "    grads[3] = grads[3].write(index=i, value=tf.transpose(fc1) @ e)\n",
    "    grads[7] = grads[7].write(index=i, value=tf.reduce_sum(e, axis=0))\n",
    "\n",
    "    # the second last fc\n",
    "    # we use droupout at the last second layer, then we should just update the nodes that are active\n",
    "    e = tf.multiply(e @ tf.transpose(weights['out']), tf.cast(tf.greater(fc1, 0), dtype=tf.float32)) / dropout\n",
    "    grads[2] = grads[2].write(index=i, value=tf.transpose(fc) @ e)\n",
    "    grads[6] = grads[6].write(index=i, value=tf.reduce_sum(e, axis=0))\n",
    "\n",
    "    # the last pooling layer\n",
    "    e = e @ tf.transpose(weights['wd1'])\n",
    "    e = tf.reshape(e, pool2.get_shape().as_list())\n",
    "\n",
    "    # the last conv layer\n",
    "    # unpooling get error from pooling layer\n",
    "    e = func.error_pooling2conv(e, mask2)\n",
    "\n",
    "    # multiply with the derivative of the active function on the conv layer\n",
    "    #     this one is also important this is a part from the upsampling, but \n",
    "    e = tf.multiply(e, tf.cast(tf.greater(conv2, 0), dtype=tf.float32))\n",
    "    temp1, temp2 = func.filter_gradient(e, pool1, conv2)\n",
    "    grads[1] = grads[1].write(index=i, value=temp1)\n",
    "    grads[5] = grads[5].write(index=i, value=temp2)\n",
    "\n",
    "    # conv to pool\n",
    "    e = func.error_conv2pooling(e, weights['wc2'])\n",
    "\n",
    "    # pool to the first conv\n",
    "    e = func.error_pooling2conv(e, mask1)\n",
    "    e = tf.multiply(e, tf.cast(tf.greater(conv1, 0), dtype=tf.float32))\n",
    "    temp1, temp2 = func.filter_gradient(e, inputx, conv1)\n",
    "    grads[0] = grads[0].write(index=i, value=temp1)\n",
    "    grads[4] = grads[4].write(index=i, value=temp2)\n",
    "    i += 1\n",
    "    del conv1, conv2, p1, f1, p2, f2, pool1, pool2, mask1, mask2, fc1, fc, e\n",
    "    return i, x, y, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "body2() takes 0 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b976a5c7aec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mi0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhile_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mcorrect_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhidou/Software/conda/envs/main/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\u001b[0m\n\u001b[1;32m   2621\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m     \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhidou/Software/conda/envs/main/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2454\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2456\u001b[0;31m           pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhidou/Software/conda/envs/main/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2404\u001b[0m         \u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moriginal_loop_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[0;32m-> 2406\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2408\u001b[0m       \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: body2() takes 0 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# ------------------------------define graph------------------------------------\n",
    "# Reshape input picture\n",
    "xs = tf.reshape(x, shape=[batch_size, 28, 28, 1])\n",
    "inputxs = tf.TensorArray(dtype=tf.float32, size=train_iter, clear_after_read=True).split(xs, [mini_batch] * train_iter)\n",
    "ys = tf.TensorArray(dtype=tf.float32, size=train_iter, clear_after_read=True).split(y, [mini_batch] * train_iter)\n",
    "\n",
    "grad_k_1 = tf.TensorArray(dtype=tf.float32, size=train_iter)\n",
    "grad_k_2 = tf.TensorArray(dtype=tf.float32, size=train_iter)\n",
    "grad_w_3 = tf.TensorArray(dtype=tf.float32, size=train_iter)\n",
    "grad_w_out = tf.TensorArray(dtype=tf.float32, size=train_iter)\n",
    "grad_b_1 = tf.TensorArray(dtype=tf.float32, size=train_iter)\n",
    "grad_b_2 = tf.TensorArray(dtype=tf.float32, size=train_iter)\n",
    "grad_b_3 = tf.TensorArray(dtype=tf.float32, size=train_iter)\n",
    "grad_b_out = tf.TensorArray(dtype=tf.float32, size=train_iter)\n",
    "correct_pred = tf.TensorArray(dtype=tf.float32, size=train_iter)\n",
    "grads = [grad_k_1, grad_k_2, grad_w_3, grad_w_out, grad_b_1, grad_b_2, grad_b_3, grad_b_out, correct_pred]\n",
    "\n",
    "# gradient\n",
    "i0 = tf.constant(0)\n",
    "con = lambda i, x, y, g: i < train_iter\n",
    "\n",
    "i0, inputx, ys, grads = tf.while_loop(cond=con, body=body2, loop_vars=[i0, inputxs, ys, grads], back_prop=False, parallel_iterations=1)\n",
    "correct_pred = grads[-1].stack()\n",
    "del grads[-1]\n",
    "for i in range(len(grads)):\n",
    "    grads[i] = (tf.reduce_mean(grads[i].stack(), axis=0), para[i])\n",
    "        \n",
    "# # apply gradient\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimize = opt.apply_gradients(grads)\n",
    "accuracy = tf.reduce_mean(correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280\n",
      "Training Accuracy= 0.17188\n",
      "Iter 2560\n",
      "Training Accuracy= 0.28125\n",
      "Iter 3840\n",
      "Training Accuracy= 0.51562\n",
      "Iter 5120\n",
      "Training Accuracy= 0.67969\n",
      "Iter 6400\n",
      "Training Accuracy= 0.64062\n",
      "Iter 7680\n",
      "Training Accuracy= 0.68750\n",
      "Iter 8960\n",
      "Training Accuracy= 0.76562\n",
      "Iter 10240\n",
      "Training Accuracy= 0.71875\n",
      "Iter 11520\n",
      "Training Accuracy= 0.78125\n",
      "Iter 12800\n",
      "Training Accuracy= 0.78125\n",
      "Iter 14080\n",
      "Training Accuracy= 0.81250\n",
      "Iter 15360\n",
      "Training Accuracy= 0.81250\n",
      "Iter 16640\n",
      "Training Accuracy= 0.84375\n",
      "Iter 17920\n",
      "Training Accuracy= 0.85156\n",
      "Iter 19200\n",
      "Training Accuracy= 0.84375\n",
      "Iter 20480\n",
      "Training Accuracy= 0.83594\n",
      "Iter 21760\n",
      "Training Accuracy= 0.82812\n",
      "Iter 23040\n",
      "Training Accuracy= 0.85156\n",
      "Iter 24320\n",
      "Training Accuracy= 0.87500\n",
      "Iter 25600\n",
      "Training Accuracy= 0.87500\n",
      "Iter 26880\n",
      "Training Accuracy= 0.88281\n",
      "Iter 28160\n",
      "Training Accuracy= 0.86719\n",
      "Iter 29440\n",
      "Training Accuracy= 0.88281\n",
      "Iter 30720\n",
      "Training Accuracy= 0.82812\n",
      "Iter 32000\n",
      "Training Accuracy= 0.92188\n",
      "Iter 33280\n",
      "Training Accuracy= 0.89062\n",
      "Iter 34560\n",
      "Training Accuracy= 0.90625\n",
      "Iter 35840\n",
      "Training Accuracy= 0.87500\n",
      "Iter 37120\n",
      "Training Accuracy= 0.92188\n",
      "Iter 38400\n",
      "Training Accuracy= 0.92969\n",
      "Iter 39680\n",
      "Training Accuracy= 0.90625\n",
      "Iter 40960\n",
      "Training Accuracy= 0.90625\n",
      "Iter 42240\n",
      "Training Accuracy= 0.89844\n",
      "Iter 43520\n",
      "Training Accuracy= 0.93750\n",
      "Iter 44800\n",
      "Training Accuracy= 0.89844\n",
      "Iter 46080\n",
      "Training Accuracy= 0.89844\n",
      "Iter 47360\n",
      "Training Accuracy= 0.89844\n",
      "Iter 48640\n",
      "Training Accuracy= 0.93750\n",
      "Iter 49920\n",
      "Training Accuracy= 0.87500\n",
      "Iter 51200\n",
      "Training Accuracy= 0.92188\n",
      "Iter 52480\n",
      "Training Accuracy= 0.93750\n",
      "Iter 53760\n",
      "Training Accuracy= 0.85938\n",
      "Iter 55040\n",
      "Training Accuracy= 0.83594\n",
      "Iter 56320\n",
      "Training Accuracy= 0.93750\n",
      "Iter 57600\n",
      "Training Accuracy= 0.92188\n",
      "Iter 58880\n",
      "Training Accuracy= 0.89062\n",
      "Iter 60160\n",
      "Training Accuracy= 0.92969\n",
      "Iter 61440\n",
      "Training Accuracy= 0.93750\n",
      "Iter 62720\n",
      "Training Accuracy= 0.92188\n",
      "Iter 64000\n",
      "Training Accuracy= 0.95312\n",
      "Iter 65280\n",
      "Training Accuracy= 0.92969\n",
      "Iter 66560\n",
      "Training Accuracy= 0.89844\n",
      "Iter 67840\n",
      "Training Accuracy= 0.93750\n",
      "Iter 69120\n",
      "Training Accuracy= 0.88281\n",
      "Iter 70400\n",
      "Training Accuracy= 0.92969\n",
      "Iter 71680\n",
      "Training Accuracy= 0.92969\n",
      "Iter 72960\n",
      "Training Accuracy= 0.92969\n",
      "Iter 74240\n",
      "Training Accuracy= 0.93750\n",
      "Iter 75520\n",
      "Training Accuracy= 0.85156\n",
      "Iter 76800\n",
      "Training Accuracy= 0.92969\n",
      "Iter 78080\n",
      "Training Accuracy= 0.92188\n",
      "Iter 79360\n",
      "Training Accuracy= 0.90625\n",
      "Iter 80640\n",
      "Training Accuracy= 0.90625\n",
      "Iter 81920\n",
      "Training Accuracy= 0.90625\n",
      "Iter 83200\n",
      "Training Accuracy= 0.92188\n",
      "Iter 84480\n",
      "Training Accuracy= 0.95312\n",
      "Iter 85760\n",
      "Training Accuracy= 0.94531\n",
      "Iter 87040\n",
      "Training Accuracy= 0.95312\n",
      "Iter 88320\n",
      "Training Accuracy= 0.87500\n",
      "Iter 89600\n",
      "Training Accuracy= 0.96875\n",
      "Iter 90880\n",
      "Training Accuracy= 0.92188\n",
      "Iter 92160\n",
      "Training Accuracy= 0.91406\n",
      "Iter 93440\n",
      "Training Accuracy= 0.96094\n",
      "Iter 94720\n",
      "Training Accuracy= 0.96875\n",
      "Iter 96000\n",
      "Training Accuracy= 0.96094\n",
      "Iter 97280\n",
      "Training Accuracy= 0.94531\n",
      "Iter 98560\n",
      "Training Accuracy= 0.89844\n",
      "Iter 99840\n",
      "Training Accuracy= 0.90625\n",
      "Iter 101120\n",
      "Training Accuracy= 0.93750\n",
      "Iter 102400\n",
      "Training Accuracy= 0.92969\n",
      "Iter 103680\n",
      "Training Accuracy= 0.92969\n",
      "Iter 104960\n",
      "Training Accuracy= 0.89844\n",
      "Iter 106240\n",
      "Training Accuracy= 0.95312\n",
      "Iter 107520\n",
      "Training Accuracy= 0.92969\n",
      "Iter 108800\n",
      "Training Accuracy= 0.94531\n",
      "Iter 110080\n",
      "Training Accuracy= 0.95312\n",
      "Iter 111360\n",
      "Training Accuracy= 0.93750\n",
      "Iter 112640\n",
      "Training Accuracy= 0.94531\n",
      "Iter 113920\n",
      "Training Accuracy= 0.91406\n",
      "Iter 115200\n",
      "Training Accuracy= 0.97656\n",
      "Iter 116480\n",
      "Training Accuracy= 0.94531\n",
      "Iter 117760\n",
      "Training Accuracy= 0.93750\n",
      "Iter 119040\n",
      "Training Accuracy= 0.94531\n",
      "Iter 120320\n",
      "Training Accuracy= 0.96094\n",
      "Iter 121600\n",
      "Training Accuracy= 0.90625\n",
      "Iter 122880\n",
      "Training Accuracy= 0.96875\n",
      "Iter 124160\n",
      "Training Accuracy= 0.96094\n",
      "Iter 125440\n",
      "Training Accuracy= 0.96094\n",
      "Iter 126720\n",
      "Training Accuracy= 0.92188\n",
      "Iter 128000\n",
      "Training Accuracy= 0.97656\n",
      "Iter 129280\n",
      "Training Accuracy= 0.95312\n",
      "Iter 130560\n",
      "Training Accuracy= 0.92188\n",
      "Iter 131840\n",
      "Training Accuracy= 0.94531\n",
      "Iter 133120\n",
      "Training Accuracy= 0.92188\n",
      "Iter 134400\n",
      "Training Accuracy= 0.95312\n",
      "Iter 135680\n",
      "Training Accuracy= 0.94531\n",
      "Iter 136960\n",
      "Training Accuracy= 0.91406\n",
      "Iter 138240\n",
      "Training Accuracy= 0.93750\n",
      "Iter 139520\n",
      "Training Accuracy= 0.92188\n",
      "Iter 140800\n",
      "Training Accuracy= 0.96094\n",
      "Iter 142080\n",
      "Training Accuracy= 0.96875\n",
      "Iter 143360\n",
      "Training Accuracy= 0.92969\n",
      "Iter 144640\n",
      "Training Accuracy= 0.93750\n",
      "Iter 145920\n",
      "Training Accuracy= 0.95312\n",
      "Iter 147200\n",
      "Training Accuracy= 0.93750\n",
      "Iter 148480\n",
      "Training Accuracy= 0.93750\n",
      "Iter 149760\n",
      "Training Accuracy= 0.92969\n",
      "Iter 151040\n",
      "Training Accuracy= 0.93750\n",
      "Iter 152320\n",
      "Training Accuracy= 0.96094\n",
      "Iter 153600\n",
      "Training Accuracy= 0.89844\n",
      "Iter 154880\n",
      "Training Accuracy= 0.92969\n",
      "Iter 156160\n",
      "Training Accuracy= 0.95312\n",
      "Iter 157440\n",
      "Training Accuracy= 0.96875\n",
      "Iter 158720\n",
      "Training Accuracy= 0.96875\n",
      "Iter 160000\n",
      "Training Accuracy= 0.96094\n",
      "Iter 161280\n",
      "Training Accuracy= 0.93750\n",
      "Iter 162560\n",
      "Training Accuracy= 0.92969\n",
      "Iter 163840\n",
      "Training Accuracy= 0.96875\n",
      "Iter 165120\n",
      "Training Accuracy= 0.94531\n",
      "Iter 166400\n",
      "Training Accuracy= 0.96094\n",
      "Iter 167680\n",
      "Training Accuracy= 0.94531\n",
      "Iter 168960\n",
      "Training Accuracy= 0.93750\n",
      "Iter 170240\n",
      "Training Accuracy= 0.93750\n",
      "Iter 171520\n",
      "Training Accuracy= 0.96094\n",
      "Iter 172800\n",
      "Training Accuracy= 0.93750\n",
      "Iter 174080\n",
      "Training Accuracy= 0.90625\n",
      "Iter 175360\n",
      "Training Accuracy= 0.95312\n",
      "Iter 176640\n",
      "Training Accuracy= 0.94531\n",
      "Iter 177920\n",
      "Training Accuracy= 0.95312\n",
      "Iter 179200\n",
      "Training Accuracy= 0.96875\n",
      "Iter 180480\n",
      "Training Accuracy= 0.93750\n",
      "Iter 181760\n",
      "Training Accuracy= 0.98438\n",
      "Iter 183040\n",
      "Training Accuracy= 0.98438\n",
      "Iter 184320\n",
      "Training Accuracy= 0.92969\n",
      "Iter 185600\n",
      "Training Accuracy= 0.92188\n",
      "Iter 186880\n",
      "Training Accuracy= 0.97656\n",
      "Iter 188160\n",
      "Training Accuracy= 0.98438\n",
      "Iter 189440\n",
      "Training Accuracy= 0.93750\n",
      "Iter 190720\n",
      "Training Accuracy= 0.96875\n",
      "Iter 192000\n",
      "Training Accuracy= 0.94531\n",
      "Iter 193280\n",
      "Training Accuracy= 0.96094\n",
      "Iter 194560\n",
      "Training Accuracy= 0.99219\n",
      "Iter 195840\n",
      "Training Accuracy= 0.94531\n",
      "Iter 197120\n",
      "Training Accuracy= 0.93750\n",
      "Iter 198400\n",
      "Training Accuracy= 0.96094\n",
      "Iter 199680\n",
      "Training Accuracy= 0.96875\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.96875\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# f = open('output.txt', 'w')\n",
    "# Launch the graph\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True  \n",
    "sess = tf.Session(config=config)  \n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "#     while step < 2:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        ret = sess.run(optimize, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            acc = sess.run(accuracy,feed_dict={x: batch_x,\n",
    "                                                          y: batch_y,\n",
    "                                                          keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) + \"\\nTraining Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "#     Calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:batch_size], y: mnist.test.labels[:batch_size],\n",
    "keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as f:\n",
    "    f.write('conv2 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, pa2[0,:,:,0], delimiter=', ',fmt=\"%.2f\")\n",
    "    \n",
    "with open('output.txt', 'a') as f:\n",
    "    f.write('\\npool2 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, po2[0,:,:,0], delimiter=', ',fmt=\"%.2f\")\n",
    "\n",
    "with open('output.txt', 'a') as f:\n",
    "    f.write('\\nerror3 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, ee3[0,:,:,0], delimiter=', ',fmt=\"%.2f\")\n",
    "\n",
    "with open('output.txt', 'a') as f:\n",
    "    f.write('\\nfirst pool value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, po1[0,:,:,0], delimiter=', ',fmt=\"%.2f\")\n",
    "    \n",
    "with open('output.txt', 'a') as f:\n",
    "    f.write('\\nerror4 value\\n')\n",
    "with open('output.txt', 'ab') as f:\n",
    "    np.savetxt(f, ee4[0,:,:,0], delimiter=', ',fmt=\"%.2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 784)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0012207 0.0\n",
      "0.000549316 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.00622559 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "for g, g1 in zip(grad, grad1):\n",
    "    print(np.max(g[0] - g1[0]) - np.min(g[0] - g1[0]), np.max(g[1] - g1[1]) - np.min(g[1] - g1[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------define graph------------------------------------\n",
    "# Reshape input picture\n",
    "inputx = tf.reshape(x, shape=[batch_size, 28, 28, 1])\n",
    "yi = y\n",
    "\n",
    "# Convolution Layer\n",
    "conv1 = func.conv2d(inputx, weights['wc1'], biases['bc1'])\n",
    "# Pooling (down-sampling)\n",
    "p1 = func.extract_patches(conv1, 'SAME', 2, 2)\n",
    "f1 = func.majority_frequency(p1)\n",
    "#     maxpooling\n",
    "pool1, mask1 = func.max_pool_with_mask(p1)\n",
    "\n",
    "# Convolution Layer\n",
    "conv2 = func.conv2d(pool1, weights['wc2'], biases['bc2'])\n",
    "#     Pooling (down-sampling)\n",
    "p2 = func.extract_patches(conv2, 'SAME', 2, 2)\n",
    "f2 = func.majority_frequency(p2)\n",
    "#     maxpooling\n",
    "pool2, mask2 = func.max_pool_with_mask(p2)\n",
    "\n",
    "# Fully connected layer\n",
    "# Reshape conv2 output to fit fully connected layer input\n",
    "fc = tf.reshape(pool2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "fc1 = tf.add(tf.matmul(fc, weights['wd1']), biases['bd1'])\n",
    "fc1 = tf.nn.relu(fc1)\n",
    "# Apply Dropout\n",
    "# fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "# Output, class prediction\n",
    "pred = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "correct_pred = tf.cast(tf.equal(tf.argmax(pred, 1), tf.argmax(yi, 1)), dtype=tf.float32)\n",
    "\n",
    "\n",
    "pool11 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "conv22 = func.conv2d(pool11, weights['wc2'], biases['bc2'])\n",
    "pool22 = tf.nn.max_pool(conv22, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "fcc = tf.reshape(pool22, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "fc11 = tf.add(tf.matmul(fcc, weights['wd1']), biases['bd1'])\n",
    "fc11 = tf.nn.relu(fc11)\n",
    "# Apply Dropout\n",
    "# fc11 = tf.nn.dropout(fc11, dropout)\n",
    "\n",
    "# Output, class prediction\n",
    "pred1 = tf.add(tf.matmul(fc11, weights['out']), biases['out'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------define graph------------------------------------\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "gv = opt.compute_gradients(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred1, labels=y)))\n",
    "\n",
    "opt2 = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "gv2 = opt2.compute_gradients(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)))\n",
    "# ------------------------------define gradient descent-------------------------\n",
    "\n",
    "# the last fc\n",
    "e = tf.nn.softmax(pred) - yi\n",
    "grad_w_out = tf.transpose(fc1) @ e\n",
    "grad_b_out = tf.reduce_sum(e, axis=0)\n",
    "\n",
    "# the second last fc\n",
    "# we use droupout at the last second layer, then we should just update the nodes that are active\n",
    "e = tf.multiply(e @ tf.transpose(weights['out']), tf.cast(tf.greater(fc1, 0), dtype=tf.float32)) #/ dropout\n",
    "grad_w_3 = tf.transpose(fc) @ e\n",
    "grad_b_3 = tf.reduce_sum(e, axis=0)\n",
    "\n",
    "# the last pooling layer\n",
    "e = e @ tf.transpose(weights['wd1'])\n",
    "e = tf.reshape(e, pool2.get_shape().as_list())\n",
    "\n",
    "# the last conv layer\n",
    "# unpooling get error from pooling layer\n",
    "e = func.error_pooling2conv(e, mask2)\n",
    "\n",
    "# multiply with the derivative of the active function on the conv layer\n",
    "#     this one is also important this is a part from the upsampling, but \n",
    "e = tf.multiply(e, tf.cast(tf.greater(conv2, 0), dtype=tf.float32))\n",
    "temp1, temp2 = func.filter_gradient(e, pool1, conv2)\n",
    "grad_k_2 = temp1\n",
    "grad_b_2 = temp2\n",
    "\n",
    "# conv to pool\n",
    "e = func.error_conv2pooling(e, weights['wc2'])\n",
    "\n",
    "# pool to the first conv\n",
    "e = func.error_pooling2conv(e, mask1)\n",
    "e = tf.multiply(e, tf.cast(tf.greater(conv1, 0), dtype=tf.float32))\n",
    "temp1, temp2 = func.filter_gradient(e, inputx, conv1)\n",
    "grad_k_1 = temp1\n",
    "grad_b_1 = temp2\n",
    "    \n",
    "    \n",
    "\n",
    "# gradient\n",
    "gv1 = [(grad_k_1 / batch_size, weights['wc1']), (grad_k_2 / batch_size, weights['wc2']), \n",
    "       (grad_w_3 / batch_size, weights['wd1']), (grad_w_out / batch_size, weights['out']),\n",
    "       (grad_b_1 / batch_size, biases['bc1']), (grad_b_2/ batch_size, biases['bc2']), \n",
    "       (grad_b_3 / batch_size, biases['bd1']), (grad_b_out / batch_size, biases['out'])]\n",
    "\n",
    "\n",
    "# apply gradient\n",
    "# opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# optimize = opt.apply_gradients(gv1)\n",
    "# accuracy = tf.reduce_mean(correct_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# f = open('output.txt', 'w')\n",
    "# Launch the graph \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "#     while step * batch_size < training_iters:\n",
    "    while step < 2:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "#         ret1 all from tf.  gv1 all from mime, gv2 half half\n",
    "        ret1, ret2, ret3 = sess.run([gv, gv1, gv2], feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            acc = sess.run(accuracy,feed_dict={x: batch_x,\n",
    "                                                          y: batch_y,\n",
    "                                                          keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) + \"\\nTraining Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference between tf and mine\n",
      "2826.2 0.0\n",
      "1302.44 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "5493.58 0.0\n",
      "273.807 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "difference between tf and half\n",
      "0.0 0.0\n",
      "0.00012207 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.00830078 0.0\n",
      "6.10352e-05 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "difference between mine and half\n",
      "2602.54 0.0\n",
      "977.362 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "4297.13 0.0\n",
      "244.018 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print('difference between tf and mine')\n",
    "for i, j in zip(ret1, ret2):\n",
    "    print(np.max(i[0] - j[0]), np.sum(np.abs(i[1] - j[1])))\n",
    "print('difference between tf and half')\n",
    "for i, k in zip(ret1, ret3):\n",
    "    print(np.max(i[0] - k[0]), np.sum(np.abs(i[1] - k[1])))\n",
    "print('difference between mine and half')\n",
    "for j, k in zip(ret2, ret3):\n",
    "    print(np.max(j[0] - k[0]), np.sum(np.abs(j[1] - k[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(wo - grad[1][0]) - np.min(wo - grad[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(bo - grad[3][0]) - np.min(bo - grad[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(w3 - grad[0][0]) - np.min(w3 - grad[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.9406967e-08"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(b3 - grad[2][0]) - np.min(b3 - grad[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00045776367"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(k2 - grad[4][0]) - np.min(k2 - grad[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.9182129e-05"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(b2 - grad[5][0]) - np.min(b2 - grad[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -315.516,  -274.134,  -110.675,   214.852,   469.886],\n",
       "       [ -139.565,  -354.963,   -14.112,   389.47 ,   461.621],\n",
       "       [ -124.195,  -157.435,   383.971,   661.055,   526.735],\n",
       "       [  419.112,   513.28 ,   890.624,  1040.974,  1066.86 ],\n",
       "       [  804.38 ,   680.623,   922.307,  1160.497,  1113.75 ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad[6][0][:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -315.516,  -274.134,  -110.675,   214.852,   469.886],\n",
       "       [ -139.565,  -354.963,   -14.112,   389.47 ,   461.621],\n",
       "       [ -124.195,  -157.435,   383.97 ,   661.055,   526.735],\n",
       "       [  419.112,   513.28 ,   890.624,  1040.974,  1066.86 ],\n",
       "       [  804.38 ,   680.623,   922.307,  1160.497,  1113.75 ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1[:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -208.329,  1371.998,  2677.15 , -1897.6  ,  1936.688,    44.462,\n",
       "        -160.821,   742.401,   621.907,  2912.838,  1529.066,  3433.003,\n",
       "         135.857, -1745.254,  2003.65 ,    17.914,  3355.716,  -864.316,\n",
       "         725.766,   287.84 ,  -320.119,   391.264,  2012.347,  4179.506,\n",
       "        -473.446,  1613.412,   398.612,  3576.725,  3137.878,  2353.416,\n",
       "        2589.489,   423.992], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -208.329,  1372.001,  2677.152, -1897.599,  1936.688,    44.462,\n",
       "        -160.821,   742.401,   621.907,  2912.84 ,  1529.065,  3433.004,\n",
       "         135.857, -1745.253,  2003.649,    17.914,  3355.718,  -864.316,\n",
       "         725.767,   287.84 ,  -320.119,   391.264,  2012.346,  4179.504,\n",
       "        -473.446,  1613.411,   398.612,  3576.727,  3137.877,  2353.414,\n",
       "        2589.489,   423.992], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  10.479,   40.462,  -20.684,  -68.879,  -72.81 ],\n",
       "       [   7.344,    1.926,  -51.897,  -53.197,  -39.338],\n",
       "       [  49.751,   -7.294,   -6.286,  -43.237,  -71.886],\n",
       "       [  -6.139,   14.189,  -37.93 , -101.595,  -38.394],\n",
       "       [ -29.756,  -19.656,  -55.514,  -51.635,    2.628]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k2[:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -18.696,    9.353,  -38.062,   10.527,  -15.099,  -22.246,\n",
       "        120.452,   56.936,  -12.423,  -74.125,   60.832,   52.296,\n",
       "         46.787,  -70.468,   -1.294,   55.804,   45.347,  -25.526,\n",
       "         59.663,   24.105,    7.68 ,   33.834,  -59.61 ,   34.805,\n",
       "        -28.622,   26.149,  -25.254,  -46.559,   70.685,   54.573,\n",
       "         -9.246,   26.074,   90.612,   34.13 ,  -12.593,   26.451,\n",
       "         44.761,  119.004,   33.882,  -55.635,   -5.529,   11.017,\n",
       "         13.544,   99.017,   82.256,  -29.706,  -50.963,  -51.528,\n",
       "         -6.476,   24.699,  111.793,   48.072,  109.696,   67.398,\n",
       "          3.102,    1.572,  -20.94 ,   20.755,    4.561,   11.462,\n",
       "         69.632,   -2.096,  -36.653,   53.967], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -18.696,    9.353,  -38.062,   10.527,  -15.099,  -22.246,\n",
       "        120.452,   56.936,  -12.423,  -74.125,   60.832,   52.296,\n",
       "         46.787,  -70.468,   -1.294,   55.804,   45.347,  -25.526,\n",
       "         59.663,   24.105,    7.68 ,   33.834,  -59.61 ,   34.805,\n",
       "        -28.622,   26.149,  -25.254,  -46.559,   70.685,   54.573,\n",
       "         -9.246,   26.074,   90.612,   34.13 ,  -12.593,   26.451,\n",
       "         44.761,  119.004,   33.882,  -55.635,   -5.529,   11.017,\n",
       "         13.544,   99.017,   82.256,  -29.706,  -50.963,  -51.528,\n",
       "         -6.476,   24.699,  111.793,   48.072,  109.696,   67.398,\n",
       "          3.102,    1.573,  -20.94 ,   20.755,    4.561,   11.462,\n",
       "         69.632,   -2.096,  -36.653,   53.967], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = np.array([range(1, 65)])\n",
    "tt = np.reshape(tt, [2, 2, 4,4], order='C')\n",
    "tt = np.transpose(tt, [0, 2,3,1])\n",
    "\n",
    "tt[0,0,3,0] = 3\n",
    "tt[0,2,0,0] = 13\n",
    "tt[0,2,1,0] = 13\n",
    "tt[0,2,3,0] = 11\n",
    "tt[0,3,2,0] = 11\n",
    "tt[0,3,3,0] = 11\n",
    "tt[0,0,0,1] = 18\n",
    "tt[0,1,0,1] = 18\n",
    "tt[0,1,1,1] = 18\n",
    "tt[0,0,2,1] = 23\n",
    "tt[0,0,3,1] = 23\n",
    "tt[0,2,0,1] = 30\n",
    "\n",
    "x = tf.constant(tt, dtype=tf.float32) + tf.random_normal(dtype=tf.float32, shape=[2,4,4,2])\n",
    "p = func.extract_patches(x, \"VALID\", 2, 2)\n",
    "pool1, mask = func.max_pool_with_mask(p=p)\n",
    "pool2 = tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1],padding='VALID')\n",
    "\n",
    "# x = tf.reshape(x, [4,4])\n",
    "with tf.Session() as sess:\n",
    "    retx, retp, retm = sess.run([x, pool1, mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.95 ,   3.11 ,   3.166,   2.916],\n",
       "       [  5.122,   5.5  ,   6.872,   8.112],\n",
       "       [ 12.704,  11.225,  11.753,   9.899],\n",
       "       [ 11.32 ,  13.192,  11.235,  10.378]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retx[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.5  ,   8.112],\n",
       "       [ 13.192,  11.753]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retp[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retm[0,1,1,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt1 = np.array([\n",
    "    [\n",
    "        [16, 2, 3, 13], [5,11,10,8], [9,7,6,12], [4, 14,15, 1]\n",
    "    ], \n",
    "    [\n",
    "        [16, 2, 3, 13], [5,11,10,8], [9,7,6,12], [4, 14,15, 1]\n",
    "    ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt1 = np.array([[[16, 2, 3, 13], [5,11,10,8], [9,7,6,12], [4, 14,15, 1]], [[16, 2, 3, 13], [5,11,10,8], [9,7,6,12], [4, 14,15, 1]]])\n",
    "tt1 = np.array([tt1, np.flip(tt1, axis=1)])\n",
    "tt1 = tt1.transpose([0,2,3,1])\n",
    "fk = np.array([[[0.8,0.1,-0.], [0.3,0.5,0.7],[-0.4,0,-0.2]]])\n",
    "fk = np.array([fk, np.flip(fk,axis=1)])\n",
    "fk = fk.transpose([0,2,3,1])\n",
    "fk = fk.transpose([1,2,3,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 4, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 1, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 14, 15,  1],\n",
       "       [ 9,  7,  6, 12],\n",
       "       [ 5, 11, 10,  8],\n",
       "       [16,  2,  3, 13]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt1[1,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xw = tf.constant(tt1, dtype=tf.float32)\n",
    "yw = tf.zeros(10)\n",
    "# xw = tf.split(xw, axis=0, num_or_size_splits=2)\n",
    "# for i in xw:\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    retx = sess.run(yw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'zeros_like_8:0' shape=(2, 1, 4, 4, 2) dtype=float32>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_8:0' shape=(2, 4, 4, 2) dtype=float32>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  2.  2.  2.]\n",
      " [ 2.  2.  2.  2.]\n",
      " [ 2.  2.  2.  2.]\n",
      " [ 2.  2.  2.  2.]]\n"
     ]
    }
   ],
   "source": [
    "print(retx[1,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor 'gradients_5/MatMul_10_grad/tuple/control_dependency_1:0' shape=(3136, 1024) dtype=float32>,\n",
       "  <tf.Variable 'Variable_10:0' shape=(3136, 1024) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_5/MatMul_11_grad/tuple/control_dependency_1:0' shape=(1024, 10) dtype=float32>,\n",
       "  <tf.Variable 'Variable_11:0' shape=(1024, 10) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_5/Add_10_grad/tuple/control_dependency_1:0' shape=(1024,) dtype=float32>,\n",
       "  <tf.Variable 'Variable_14:0' shape=(1024,) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_5/Add_11_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>,\n",
       "  <tf.Variable 'Variable_15:0' shape=(10,) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_5/Conv2D_16_grad/tuple/control_dependency_1:0' shape=(5, 5, 32, 64) dtype=float32>,\n",
       "  <tf.Variable 'Variable_9:0' shape=(5, 5, 32, 64) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_5/BiasAdd_11_grad/tuple/control_dependency_1:0' shape=(64,) dtype=float32>,\n",
       "  <tf.Variable 'Variable_13:0' shape=(64,) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_5/Conv2D_15_grad/tuple/control_dependency_1:0' shape=(5, 5, 1, 32) dtype=float32>,\n",
       "  <tf.Variable 'Variable_8:0' shape=(5, 5, 1, 32) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_5/BiasAdd_10_grad/tuple/control_dependency_1:0' shape=(32,) dtype=float32>,\n",
       "  <tf.Variable 'Variable_12:0' shape=(32,) dtype=float32_ref>)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.ops.variables.Variable"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gv[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = tf.placeholder(tf.float32, [2,3,3])\n",
    "xa = tf.TensorArray(dtype=tf.int32, size=2, clear_after_read=True).split(value=xs, lengths=[1] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xs = tf.placeholder(tf.float32, [2,3,3])\n",
    "ys = tf.constant([[[1,2,3],[4,5,6],[7,8,9]],[[10,11,12],[13,14,15],[16,17,18]]])\n",
    "# zs = tf.zeros_like(xs)\n",
    "\n",
    "xa = tf.TensorArray(dtype=tf.int32, size=2, clear_after_read=True).split(value=xs, lengths=[1,1])\n",
    "ya = tf.TensorArray(dtype=tf.int32, size=2, clear_after_read=True).split(value=ys, lengths=[1,1])\n",
    "za = tf.TensorArray(dtype=tf.int32, size=2)\n",
    "# za = za.write(index=i0, value=xa.read(index=i0) + ya.read(index=i0))\n",
    "# temp = za.read(index=i0)\n",
    "\n",
    "i0 = tf.constant(0)\n",
    "\n",
    "con = lambda i, x, y, z: i < 2\n",
    "\n",
    "def body(i, x, y, z):\n",
    "    z = z.write(index=i, value=x.read(index=i) + y.read(index=i))\n",
    "    i += 1\n",
    "    return i, x, y, z\n",
    "\n",
    "i0, xa, ya, za = tf.while_loop(cond=con, body=body, loop_vars=[i0, xa, ya, za], back_prop=False, parallel_iterations=1)\n",
    "temp = za.read(index=tf.constant(0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True  \n",
    "sess = tf.Session(config=config) \n",
    "with tf.Session(config=config) as sess:\n",
    "    ret = sess.run(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2,  4,  6],\n",
       "        [ 8, 10, 12],\n",
       "        [14, 16, 18]]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def foo():\n",
    "    global i\n",
    "    i += 1\n",
    "foo()\n",
    "foo()\n",
    "print(i)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
